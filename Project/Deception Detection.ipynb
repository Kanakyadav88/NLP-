{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11402509,"sourceType":"datasetVersion","datasetId":7141907},{"sourceId":11402928,"sourceType":"datasetVersion","datasetId":7142230}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% Cell 1: Imports and Setup\nimport os\nimport json\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertModel, DistilBertTokenizer  # Using DistilBERT\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport sklearn.metrics\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"[Setup] Using device: {device}\")\n\n# Set batch size based on device\nif device.type == \"cuda\":\n    batch_size = 16\nelse:\n    batch_size = 4\nprint(f\"[Setup] Using batch size: {batch_size}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:16:57.663119Z","iopub.execute_input":"2025-04-14T13:16:57.663313Z","iopub.status.idle":"2025-04-14T13:17:23.124653Z","shell.execute_reply.started":"2025-04-14T13:16:57.663296Z","shell.execute_reply":"2025-04-14T13:17:23.124027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% Cell 2: Model and Dataset Definitions\n\n# ------------------------------\n# Define the DistilBERT-based model.\n# ------------------------------\nclass DeceptionBERTModel(nn.Module):\n    def __init__(self, distilbert_model_name=\"distilbert-base-uncased\", metadata_dim=9):\n        super(DeceptionBERTModel, self).__init__()\n        \n        print(\"[Model] Loading DistilBERT model...\")\n        self.bert = DistilBertModel.from_pretrained(distilbert_model_name)\n        self.bert_dim = self.bert.config.dim  # DistilBERT's hidden size\n        \n        # Layers for processing metadata\n        self.metadata_fc = nn.Sequential(\n            nn.Linear(metadata_dim, 128),  # Increased from 64 to 128\n            nn.ReLU(),\n            nn.Dropout(0.3),  # Increased dropout from 0.2 to 0.3\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n        )\n        \n        # Combined layers with higher dropout\n        self.combined_fc = nn.Sequential(\n            nn.Linear(self.bert_dim + 64, 512),  # Increased from 256 to 512\n            nn.ReLU(),\n            nn.Dropout(0.4),  # Increased from 0.3 to 0.4\n            nn.Linear(512, 128),  # Added extra layer\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),  # Increased dropout from 0.2 to 0.3\n            nn.Linear(64, 1)\n        )\n        print(\"[Model] Model architecture defined.\")\n\n    def forward(self, input_ids, attention_mask, metadata):\n        # Get DistilBERT embeddings\n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_embedding = bert_outputs.last_hidden_state[:, 0, :]  # take first token ([CLS]-like)\n        \n        # Process metadata\n        metadata_embedding = self.metadata_fc(metadata)\n        \n        # Combine embeddings\n        combined_embedding = torch.cat((cls_embedding, metadata_embedding), dim=1)\n        \n        # Final prediction (logit output)\n        output = self.combined_fc(combined_embedding)\n        \n        return output\n\n\n# ------------------------------\n# Define the custom dataset class.\n# ------------------------------\nclass DeceptionDataset(Dataset):\n    def __init__(self, texts, metadata, labels=None, tokenizer=None, max_length=64):\n        self.texts = texts\n        self.metadata = metadata\n        self.labels = labels  # these will be the \"is_deceptive\" labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        input_ids = encoding[\"input_ids\"].squeeze(0)\n        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n        metadata = torch.tensor(self.metadata[idx], dtype=torch.float)\n\n        item = {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"metadata\": metadata\n        }\n        if self.labels is not None:\n            item[\"label\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n        return item\n\nprint(\"[ Model and dataset classes loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:19:44.590190Z","iopub.execute_input":"2025-04-14T13:19:44.590475Z","iopub.status.idle":"2025-04-14T13:19:44.601412Z","shell.execute_reply.started":"2025-04-14T13:19:44.590453Z","shell.execute_reply":"2025-04-14T13:19:44.600530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% Cell 3: Data Loading and Preprocessing Functions\n\ndef load_processed_data(data_dir=\"processed_data_balanced\"):\n    \"\"\"Load the processed and balanced data from CSV files.\"\"\"\n    print(\"[Data] Loading processed data...\")\n    \n    train_df = pd.read_csv(\"/kaggle/input/processed-data/processed_train_balanced.csv\")\n    val_df = pd.read_csv(\"/kaggle/input/processed-data/processed_val.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/processed-data/processed_test.csv\")\n    \n    print(f\"[Data] Train data shape: {train_df.shape}\")\n    print(f\"[Data] Validation data shape: {val_df.shape}\")\n    print(f\"[Data] Test data shape: {test_df.shape}\")\n    \n    # Print original balance based on 'is_truthful'\n    train_truthful = train_df[\"is_truthful\"].sum()\n    train_deceptive = len(train_df) - train_truthful\n    print(f\"[Data] Original training class balance (truthful): {train_truthful} truthful, {train_deceptive} deceptive\")\n    print(f\"[Data] Truthful percentage: {train_truthful / len(train_df) * 100:.2f}%\")\n    \n    return train_df, val_df, test_df\n\ndef get_metadata_features(df):\n    \"\"\"Get metadata features from the dataframe.\"\"\"\n    metadata_features = [\n        \"message_length\",\n        \"word_count\",\n        \"question_count\",\n        \"exclamation_count\",\n        \"has_uncertainty\",\n        \"has_certainty\",\n        \"conversation_length\",\n        \"msg_position_in_convo\",\n        \"position_ratio\"\n    ]\n    if \"sender_is_player\" in df.columns:\n        metadata_features.append(\"sender_is_player\")\n    if \"prev_msg_truthful\" in df.columns:\n        metadata_features.append(\"prev_msg_truthful\")\n    if \"game_stage\" in df.columns:\n        metadata_features.append(\"game_stage\")\n    \n    print(f\"[Data] Selected metadata features: {metadata_features}\")\n    return metadata_features\n\ndef create_datasets(train_df, val_df, test_df, tokenizer, metadata_features):\n    \"\"\"Create datasets for training, validation, and testing.\"\"\"\n    print(\"[Data] Creating datasets...\")\n    \n    # Get texts from cleaned_message column\n    train_texts = train_df[\"cleaned_message\"].fillna(\"\").values\n    val_texts = val_df[\"cleaned_message\"].fillna(\"\").values\n    test_texts = test_df[\"cleaned_message\"].fillna(\"\").values\n\n    # ------------------------------------------------------------\n    # UPDATED LABEL HANDLING:\n    # Convert the original 'is_truthful' to 'is_deceptive'\n    # 1 indicates deceptive (minority), 0 indicates truthful.\n    # ------------------------------------------------------------\n    train_labels = (1 - train_df[\"is_truthful\"]).values\n    val_labels = (1 - val_df[\"is_truthful\"]).values\n    test_labels = (1 - test_df[\"is_truthful\"]).values\n    \n    # Convert metadata columns to numeric if necessary\n    for col in metadata_features:\n        for df in [train_df, val_df, test_df]:\n            if col in df.columns:\n                if (df[col].dtype == bool) or (df[col].dtype == \"object\" and df[col].isin([True, False, \"True\", \"False\"]).all()):\n                    df[col] = df[col].map({True: 1, False: 0, \"True\": 1, \"False\": 0})\n                if df[col].dtype == \"object\":\n                    try:\n                        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n                    except Exception as e:\n                        print(f\"[Data] Converting column {col} to numeric, error: {e}\")\n                        df[col] = df[col].astype(\"category\").cat.codes\n\n    print(f\"[Data] Using {len(metadata_features)} metadata features: {metadata_features}\")\n    train_metadata = train_df[metadata_features].values\n    val_metadata = val_df[metadata_features].values\n    test_metadata = test_df[metadata_features].values\n\n    # Ensure all metadata columns are numeric and apply scaling.\n    for i, feature in enumerate(metadata_features):\n        for data, name in [(train_metadata, \"train\"), (val_metadata, \"val\"), (test_metadata, \"test\")]:\n            if not np.issubdtype(data[:, i].dtype, np.number):\n                print(f\"[Data] Warning: {name} feature '{feature}' is not numeric; converting.\")\n                data[:, i] = np.array([float(str(x).replace(\"True\", \"1\").replace(\"False\", \"0\")) for x in data[:, i]])\n    \n    metadata_scaler = StandardScaler()\n    train_metadata = metadata_scaler.fit_transform(train_metadata)\n    val_metadata = metadata_scaler.transform(val_metadata)\n    test_metadata = metadata_scaler.transform(test_metadata)\n    \n    joblib.dump(metadata_scaler, \"models/bert_full_metadata_scaler.pkl\")\n    print(\"[Data] Metadata scaling complete and scaler saved.\")\n    \n    train_dataset = DeceptionDataset(train_texts, train_metadata, train_labels, tokenizer)\n    val_dataset = DeceptionDataset(val_texts, val_metadata, val_labels, tokenizer)\n    test_dataset = DeceptionDataset(test_texts, test_metadata, test_labels, tokenizer)\n    \n    print(\"[Data] Datasets created successfully.\")\n    return train_dataset, val_dataset, test_dataset\n\nprint(\" Data loading and preprocessing functions defined.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:22:40.009416Z","iopub.execute_input":"2025-04-14T13:22:40.010145Z","iopub.status.idle":"2025-04-14T13:22:40.022218Z","shell.execute_reply.started":"2025-04-14T13:22:40.010118Z","shell.execute_reply":"2025-04-14T13:22:40.021550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% Cell 4: Training and Evaluation Functions\n\ndef train_model(model, train_dataset, val_dataset, device, batch_size=16, num_epochs=3, learning_rate=2e-5):\n    print(\"[Train] Starting training...\")\n    \n    # Calculate class weights based on new 'is_deceptive' labels.\n    train_labels = np.array([item[\"label\"].item() for item in train_dataset])\n    deceptive_count = np.sum(train_labels)\n    truthful_count = len(train_labels) - deceptive_count\n    print(f\"[Train] Deceptive count: {deceptive_count}, Truthful count: {truthful_count}\")\n    \n    # Use pos_weight to emphasize the deceptive (minority) class (label 1)\n    weight_for_deceptive = 3.0\n    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([weight_for_deceptive]).to(device))\n    print(f\"[Train] Using loss pos_weight: {weight_for_deceptive} for deceptive class.\")\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=learning_rate, total_steps=len(train_loader) * num_epochs\n    )\n\n    best_val_f1 = 0.0\n    patience = 2\n    epochs_no_improve = 0\n    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"val_f1\": []}\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        print(f\"[Train] Epoch {epoch+1}/{num_epochs} - Training started.\")\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            metadata = batch[\"metadata\"].to(device)\n            labels = batch[\"label\"].to(device).unsqueeze(1)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask, metadata)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            train_loss += loss.item() * input_ids.size(0)\n            train_total += input_ids.size(0)\n            preds = (torch.sigmoid(outputs) >= 0.5).float()  # using consistent threshold 0.5\n            train_correct += (preds == labels).sum().item()\n        \n        train_loss = train_loss / train_total\n        train_acc = train_correct / train_total\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        \n        print(f\"[Train] Epoch {epoch+1} completed. Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_preds = []\n        val_labels_list = []\n        print(f\"[Val] Starting validation for Epoch {epoch+1}...\")\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Val)\"):\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n                metadata = batch[\"metadata\"].to(device)\n                labels = batch[\"label\"].to(device).unsqueeze(1)\n                outputs = model(input_ids, attention_mask, metadata)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * input_ids.size(0)\n                preds = (torch.sigmoid(outputs) >= 0.5).float()\n                val_preds.extend(preds.cpu().numpy())\n                val_labels_list.extend(labels.cpu().numpy())\n        val_loss = val_loss / len(val_dataset)\n        val_preds = np.array(val_preds).flatten()\n        val_labels_list = np.array(val_labels_list).flatten()\n        val_acc = accuracy_score(val_labels_list, val_preds)\n        val_f1 = f1_score(val_labels_list, val_preds)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        history[\"val_f1\"].append(val_f1)\n        \n        print(f\"[Val] Epoch {epoch+1}: Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n        \n        # Early stopping check\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), \"models/bert_full_model_best.pt\")\n            print(f\"[Train] Model improved! Best F1 updated: {best_val_f1:.4f}\")\n        else:\n            epochs_no_improve += 1\n            print(f\"[Train] No improvement for {epochs_no_improve} epoch(s).\")\n        if epochs_no_improve >= patience:\n            print(f\"[Train] Early stopping triggered at epoch {epoch+1}.\")\n            break\n\n    # Plot learning curves\n    print(\"[Train] Plotting learning curves...\")\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.plot(history[\"train_loss\"], label=\"Train\")\n    plt.plot(history[\"val_loss\"], label=\"Validation\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.subplot(1, 3, 2)\n    plt.plot(history[\"train_acc\"], label=\"Train\")\n    plt.plot(history[\"val_acc\"], label=\"Validation\")\n    plt.title(\"Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.subplot(1, 3, 3)\n    plt.plot(history[\"val_f1\"], label=\"Validation\")\n    plt.title(\"F1 Score\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"F1 Score\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(\"results/bert_full_learning_curves.png\")\n    plt.close()\n    \n    model.load_state_dict(torch.load(\"models/bert_full_model_best.pt\"))\n    print(f\"[Train] Loaded best model with validation F1: {best_val_f1:.4f}\")\n    return model\n\ndef evaluate_model(model, test_dataset, device, batch_size=16):\n    print(\"[Evaluate] Evaluating model on test set...\")\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    model.eval()\n    test_preds = []\n    test_probs = []\n    test_labels_list = []\n    \n    threshold = 0.5  # consistent threshold for evaluation\n    print(f\"[Evaluate] Using threshold: {threshold}\")\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            metadata = batch[\"metadata\"].to(device)\n            labels = batch[\"label\"].to(device).unsqueeze(1)\n            outputs = model(input_ids, attention_mask, metadata)\n            probs = torch.sigmoid(outputs)\n            preds = (probs >= threshold).float()\n            test_preds.extend(preds.cpu().numpy())\n            test_probs.extend(probs.cpu().numpy())\n            test_labels_list.extend(labels.cpu().numpy())\n    \n    test_preds = np.array(test_preds).flatten()\n    test_probs = np.array(test_probs).flatten()\n    test_labels_list = np.array(test_labels_list).flatten()\n    metrics = calculate_metrics(test_labels_list, test_preds, test_probs, threshold)\n    \n    df_preds = pd.DataFrame({\n        \"prediction\": test_preds,\n        \"probability\": test_probs,\n        \"true_label\": test_labels_list\n    })\n    df_preds.to_csv(\"results/bert_full_predictions.csv\", index=False)\n    \n    calibrate_threshold(test_labels_list, test_probs)\n    return metrics\n\ndef calculate_metrics(y_true, y_pred, y_prob, threshold=0.5):\n    print(\"[Metrics] Calculating metrics...\")\n    acc = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Adjust confusion matrix shape if necessary\n    if cm.shape != (2, 2):\n        if cm.shape == (1, 1):\n            if y_true[0] == 1:\n                cm = np.array([[0, 0], [0, cm[0, 0]]])\n            else:\n                cm = np.array([[cm[0, 0], 0], [0, 0]])\n    tn, fp, fn, tp = cm.ravel()\n    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n    sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n    balanced_acc = (specificity + sensitivity) / 2\n    print(f\"[Metrics] Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n    print(f\"[Metrics] Specificity: {specificity:.4f}, Sensitivity: {sensitivity:.4f}\")\n    print(f\"[Metrics] Confusion Matrix:\\n{cm}\")\n    \n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Truthful\", \"Deceptive\"], yticklabels=[\"Truthful\", \"Deceptive\"])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(\"results/bert_full_confusion_matrix.png\")\n    plt.close()\n    \n    with open(\"results/bert_full_metrics.json\", \"w\") as f:\n        json.dump({\n            \"accuracy\": float(acc),\n            \"balanced_accuracy\": float(balanced_acc),\n            \"precision\": float(precision),\n            \"recall\": float(recall),\n            \"f1\": float(f1),\n            \"specificity\": float(specificity),\n            \"sensitivity\": float(sensitivity),\n            \"threshold\": float(threshold),\n            \"confusion_matrix\": cm.tolist()\n        }, f, indent=4)\n    \n    return {\n        \"accuracy\": acc,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"specificity\": specificity,\n        \"sensitivity\": sensitivity,\n        \"balanced_accuracy\": balanced_acc,\n        \"confusion_matrix\": cm\n    }\n\ndef calibrate_threshold(y_true, y_prob):\n    print(\"[Calibrate] Calibrating classification threshold...\")\n    fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_true, y_prob)\n    roc_auc = sklearn.metrics.auc(fpr, tpr)\n    gmeans = np.sqrt(tpr * (1 - fpr))\n    ix = np.argmax(gmeans)\n    best_threshold = thresholds[ix]\n    y_pred_best = (y_prob >= best_threshold).astype(int)\n    cm_best = confusion_matrix(y_true, y_pred_best)\n    acc_best = accuracy_score(y_true, y_pred_best)\n    precision_best = precision_score(y_true, y_pred_best, zero_division=0)\n    recall_best = recall_score(y_true, y_pred_best, zero_division=0)\n    f1_best = f1_score(y_true, y_pred_best, zero_division=0)\n    \n    plt.figure(figsize=(10, 8))\n    plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.4f})\")\n    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n    plt.scatter(fpr[ix], tpr[ix], marker=\"o\", color=\"black\", label=f\"Best threshold = {best_threshold:.4f}\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n    plt.legend(loc=\"lower right\")\n    plt.savefig(\"results/bert_full_roc_curve.png\")\n    plt.close()\n    \n    print(f\"[Calibrate] Best threshold: {best_threshold:.4f}\")\n    print(f\"[Calibrate] Accuracy: {acc_best:.4f}, F1: {f1_best:.4f}\")\n    print(f\"[Calibrate] Confusion Matrix with best threshold:\\n{cm_best}\")\n    \n    with open(\"results/bert_full_calibration.json\", \"w\") as f:\n        json.dump({\n            \"best_threshold\": float(best_threshold),\n            \"accuracy\": float(acc_best),\n            \"precision\": float(precision_best),\n            \"recall\": float(recall_best),\n            \"f1\": float(f1_best),\n            \"roc_auc\": float(roc_auc),\n            \"confusion_matrix\": cm_best.tolist()\n        }, f, indent=4)\n    return best_threshold\n\nprint(\"[Cell 4] Training and evaluation functions defined.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:23:14.897392Z","iopub.execute_input":"2025-04-14T13:23:14.897890Z","iopub.status.idle":"2025-04-14T13:23:14.926170Z","shell.execute_reply.started":"2025-04-14T13:23:14.897862Z","shell.execute_reply":"2025-04-14T13:23:14.925376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% Cell 5: Main Routine\n\ndef main():\n    os.makedirs(\"models\", exist_ok=True)\n    os.makedirs(\"results\", exist_ok=True)\n    \n    print(\"[Main] Loading processed data...\")\n    train_df, val_df, test_df = load_processed_data()\n    \n    # ---------------------------------------------------\n    # Balance the training data by oversampling the deceptive class.\n    # Use the original is_truthful field and later invert to get is_deceptive.\n    # ---------------------------------------------------\n    truthful_df = train_df[train_df[\"is_truthful\"] == 1]\n    deceptive_df = train_df[train_df[\"is_truthful\"] == 0]\n    n_to_sample = len(truthful_df) - len(deceptive_df)\n    print(f\"[Main] Oversampling deceptive data: need {n_to_sample} extra samples.\")\n    oversampled_deceptive = deceptive_df.sample(n_to_sample, replace=True, random_state=42)\n    train_df = pd.concat([truthful_df, deceptive_df, oversampled_deceptive])\n    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n    print(f\"[Main] Balanced train data shape: {train_df.shape}\")\n    print(f\"[Main] Validation data shape: {val_df.shape}\")\n    print(f\"[Main] Test data shape: {test_df.shape}\")\n    train_truthful = train_df[\"is_truthful\"].sum()\n    train_deceptive = len(train_df) - train_truthful\n    print(f\"[Main] After balancing: {train_truthful} truthful, {train_deceptive} deceptive (original labels)\")\n    print(f\"[Main] Truthful percentage: {train_truthful / len(train_df) * 100:.2f}%\")\n    \n    print(\"[Main] Initializing tokenizer...\")\n    model_name = \"distilbert-base-uncased\"\n    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n    print(f\"[Main] Using tokenizer from model: {model_name}\")\n    \n    metadata_features = get_metadata_features(train_df)\n    train_dataset, val_dataset, test_dataset = create_datasets(train_df, val_df, test_df, tokenizer, metadata_features)\n    \n    print(\"[Main] Initializing model...\")\n    model = DeceptionBERTModel(model_name, metadata_dim=len(metadata_features))\n    model = model.to(device)\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"[Main] Total parameters: {total_params:,}\")\n    print(f\"[Main] Trainable parameters: {trainable_params:,}\")\n    print(f\"[Main] Percentage trainable: {trainable_params / total_params * 100:.2f}%\")\n    \n    print(\"[Main] Starting training process...\")\n    model = train_model(model, train_dataset, val_dataset, device, batch_size, num_epochs=3)\n    \n    print(\"[Main] Evaluating model on test set...\")\n    evaluate_model(model, test_dataset, device, batch_size)\n    \n    print(\"[Main] Saving final model and tokenizer...\")\n    torch.save(model.state_dict(), \"models/bert_full_model.pt\")\n    os.makedirs(\"models/bert_full_tokenizer\", exist_ok=True)\n    tokenizer.save_pretrained(\"models/bert_full_tokenizer\")\n    print(\"[Main] Process completed.\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:48:53.948088Z","iopub.execute_input":"2025-04-21T10:48:53.948770Z","iopub.status.idle":"2025-04-21T10:48:53.986982Z","shell.execute_reply.started":"2025-04-21T10:48:53.948746Z","shell.execute_reply":"2025-04-21T10:48:53.986075Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Updating based on the above results**","metadata":{}},{"cell_type":"code","source":"# %% Cell 1: Imports and Setup\nimport os\nimport json\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaModel, RobertaTokenizer  # Use RoBERTa for advanced representations\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport sklearn.metrics\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"[Setup] Using device: {device}\")\n\n# Set batch size based on device\nif device.type == \"cuda\":\n    batch_size = 16\nelse:\n    batch_size = 4\nprint(f\"[Setup] Using batch size: {batch_size}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T23:34:57.512790Z","iopub.execute_input":"2025-04-20T23:34:57.513469Z","iopub.status.idle":"2025-04-20T23:34:57.524098Z","shell.execute_reply.started":"2025-04-20T23:34:57.513444Z","shell.execute_reply":"2025-04-20T23:34:57.523374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# --- Define Focal Loss ---\nclass FocalLoss(nn.Module):\n    \"\"\"\n    Focal Loss for binary classification.\n    alpha: weight for positive class.\n    gamma: focusing parameter.\n    reduction: reduction method ('mean' or 'sum').\n    \"\"\"\n    def __init__(self, alpha=1, gamma=2, reduction=\"mean\"):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        \n    def forward(self, inputs, targets):\n        # inputs: logits; targets: float tensor of 0s and 1s\n        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n        probs = torch.sigmoid(inputs)\n        p_t = targets * probs + (1 - targets) * (1 - probs)\n        loss = self.alpha * (1 - p_t) ** self.gamma * bce_loss\n        if self.reduction == \"mean\":\n            return loss.mean()\n        elif self.reduction == \"sum\":\n            return loss.sum()\n        else:\n            return loss\n\nprint(\"[Cell 2] FocalLoss defined.\")\n\n# --- Hierarchical Deception Model ---\nclass HierarchicalDeceptionModel(nn.Module):\n    def __init__(self, roberta_model_name=\"roberta-base\", metadata_dim=9, \n                 hidden_size=768, lstm_hidden_size=256, num_context_msgs=5):\n        \"\"\"\n        roberta_model_name: name of the transformer model.\n        metadata_dim: dimension of metadata features.\n        hidden_size: hidden size of RoBERTa (768 for roberta-base).\n        lstm_hidden_size: hidden size for the context LSTM.\n        num_context_msgs: maximum number of context messages.\n        \"\"\"\n        super(HierarchicalDeceptionModel, self).__init__()\n        print(\"[Model] Loading RoBERTa model...\")\n        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n        self.roberta_hidden = self.roberta.config.hidden_size\n        self.num_context_msgs = num_context_msgs\n        \n        # For encoding each context message individually\n        # We reuse the same roberta for context messages\n        \n        # LSTM to aggregate context embeddings (each message’s [CLS] token)\n        self.context_lstm = nn.LSTM(input_size=self.roberta_hidden, hidden_size=lstm_hidden_size,\n                                    batch_first=True, bidirectional=True)\n        self.lstm_out_dim = lstm_hidden_size * 2  # because bidirectional\n        \n        # Process current message (encoded using roberta)\n        # We use the [CLS] representation of the current message directly.\n        \n        # Process metadata features\n        self.metadata_fc = nn.Sequential(\n            nn.Linear(metadata_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        \n        # Final fusion: concatenate current message, context, and metadata\n        fusion_dim = self.roberta_hidden + self.lstm_out_dim + 64\n        self.fusion_fc = nn.Sequential(\n            nn.Linear(fusion_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 1)\n        )\n        \n        print(\"[Model] HierarchicalDeceptionModel initialized.\")\n    \n    def forward(self, current_input_ids, current_attention_mask, \n                context_input_ids, context_attention_mask, metadata):\n        \"\"\"\n        current_input_ids, current_attention_mask: for the current message.\n        context_input_ids: [batch, num_context_msgs, seq_len]\n        context_attention_mask: [batch, num_context_msgs, seq_len]\n        metadata: [batch, metadata_dim]\n        \"\"\"\n        # Encode current message\n        current_outputs = self.roberta(input_ids=current_input_ids, \n                                       attention_mask=current_attention_mask)\n        current_embedding = current_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n        \n        # Encode context messages in a loop (or using flatten and reshape)\n        batch_size, num_context, seq_len = context_input_ids.shape\n        # Merge batch and context dims for encoding\n        flat_context_ids = context_input_ids.view(-1, seq_len)\n        flat_context_mask = context_attention_mask.view(-1, seq_len)\n        context_outputs = self.roberta(input_ids=flat_context_ids,\n                                       attention_mask=flat_context_mask)\n        # Get [CLS] equivalent for each context message\n        context_embeddings = context_outputs.last_hidden_state[:, 0, :]  # shape [batch*num_context, hidden]\n        # Reshape back to [batch, num_context, hidden]\n        context_embeddings = context_embeddings.view(batch_size, num_context, -1)\n        \n        # Process through LSTM: obtain last hidden state (or use mean pooling)\n        lstm_out, (h_n, c_n) = self.context_lstm(context_embeddings)\n        # Concatenate the final states from both directions\n        # Alternatively, use mean across time steps:\n        context_rep = lstm_out.mean(dim=1)  # shape [batch, lstm_out_dim]\n        \n        # Process metadata\n        meta_out = self.metadata_fc(metadata)\n        \n        # Fusion: concatenate current message, context representation, and metadata\n        fusion_input = torch.cat((current_embedding, context_rep, meta_out), dim=1)\n        output = self.fusion_fc(fusion_input)\n        return output\n\nprint(\"[Cell 2] HierarchicalDeceptionModel defined.\")\n\n# --- Hierarchical Dataset ---\nclass HierarchicalDeceptionDataset(Dataset):\n    def __init__(self, texts, context_texts, metadata, labels=None, tokenizer=None, \n                 max_length=64, max_context=5, context_delim=\"||\"):\n        \"\"\"\n        texts: current message texts.\n        context_texts: context field (each entry is a string with previous messages separated by context_delim).\n        metadata: metadata features.\n        labels: target labels.\n        max_length: maximum token length per message.\n        max_context: maximum number of context messages to use.\n        context_delim: delimiter used in the context field.\n        \"\"\"\n        self.texts = texts\n        self.context_texts = context_texts\n        self.metadata = metadata\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.max_context = max_context\n        self.context_delim = context_delim\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        # Process current message\n        current_text = str(self.texts[idx])\n        current_enc = self.tokenizer(\n            current_text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        current_input_ids = current_enc[\"input_ids\"].squeeze(0)\n        current_attention_mask = current_enc[\"attention_mask\"].squeeze(0)\n        \n        # Process context: if context_texts is missing or empty, use a list with one empty string.\n        context_str = str(self.context_texts[idx]) if self.context_texts is not None else \"\"\n        # Split by delimiter. If no context exists, provide empty message(s)\n        context_msgs = [msg.strip() for msg in context_str.split(self.context_delim) if msg.strip()]\n        # Limit number of context messages\n        context_msgs = context_msgs[-self.max_context:]  # use last max_context messages\n        # If not enough messages, pad with empty strings\n        while len(context_msgs) < self.max_context:\n            context_msgs.insert(0, \"\")\n        \n        # Tokenize each context message\n        context_encodings = [self.tokenizer(\n            msg,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        ) for msg in context_msgs]\n        # Stack input_ids and attention masks along new dimension: shape [max_context, seq_len]\n        context_input_ids = torch.stack([enc[\"input_ids\"].squeeze(0) for enc in context_encodings])\n        context_attention_mask = torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T23:34:57.625635Z","iopub.execute_input":"2025-04-20T23:34:57.626178Z","iopub.status.idle":"2025-04-20T23:34:57.648379Z","shell.execute_reply.started":"2025-04-20T23:34:57.626151Z","shell.execute_reply":"2025-04-20T23:34:57.647691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Cell 2: HierarchicalDeceptionDataset Definition with Error Handling\n\nclass HierarchicalDeceptionDataset(Dataset):\n    def __init__(self, texts, context_texts, metadata, labels=None, tokenizer=None, \n                 max_length=64, max_context=5, context_delim=\"||\"):\n        \"\"\"\n        texts: current message texts.\n        context_texts: context field (each entry is a string with previous messages separated by context_delim).\n        metadata: metadata features.\n        labels: target labels.\n        max_length: maximum token length per message.\n        max_context: maximum number of context messages to use.\n        context_delim: delimiter used in the context field.\n        \"\"\"\n        self.texts = texts\n        self.context_texts = context_texts\n        self.metadata = metadata\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.max_context = max_context\n        self.context_delim = context_delim\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        # Ensure current text is not None or NaN:\n        current_text = self.texts[idx]\n        if current_text is None or (isinstance(current_text, float) and np.isnan(current_text)):\n            # If missing, default to empty string.\n            current_text = \"\"\n            # Uncomment next line for debug if needed:\n            # print(f\"Warning: Missing current message at index {idx}, defaulting to empty string.\")\n        else:\n            current_text = str(current_text)\n            \n        current_enc = self.tokenizer(\n            current_text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        current_input_ids = current_enc[\"input_ids\"].squeeze(0)\n        current_attention_mask = current_enc[\"attention_mask\"].squeeze(0)\n        \n        # Process context: Check if context_texts is None or if this particular value is missing.\n        context_entry = self.context_texts[idx]\n        if context_entry is None or (isinstance(context_entry, float) and np.isnan(context_entry)):\n            context_str = \"\"\n            # Debug print if desired:\n            # print(f\"Warning: Missing context at index {idx}, defaulting to empty string.\")\n        else:\n            context_str = str(context_entry)\n            \n        # Split context messages by delimiter; if empty, it remains empty.\n        context_msgs = [msg.strip() for msg in context_str.split(self.context_delim) if msg.strip()]\n        # Limit to the last max_context messages\n        context_msgs = context_msgs[-self.max_context:]\n        # Pad with empty strings if fewer messages than max_context\n        while len(context_msgs) < self.max_context:\n            context_msgs.insert(0, \"\")\n        \n        # Tokenize each context message\n        context_encodings = []\n        for msg in context_msgs:\n            # If the message is empty, still tokenize to avoid None.\n            msg = msg if msg is not None else \"\"\n            enc = self.tokenizer(\n                msg,\n                add_special_tokens=True,\n                max_length=self.max_length,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\"\n            )\n            context_encodings.append(enc)\n        \n        context_input_ids = torch.stack([enc[\"input_ids\"].squeeze(0) for enc in context_encodings])\n        context_attention_mask = torch.stack([enc[\"attention_mask\"].squeeze(0) for enc in context_encodings])\n        \n        # Process metadata (if any value is missing, this step should have been handled in preprocessing)\n        meta = torch.tensor(self.metadata[idx], dtype=torch.float)\n        \n        item = {\n            \"current_input_ids\": current_input_ids,\n            \"current_attention_mask\": current_attention_mask,\n            \"context_input_ids\": context_input_ids,  # shape [max_context, seq_len]\n            \"context_attention_mask\": context_attention_mask,\n            \"metadata\": meta,\n        }\n        if self.labels is not None:\n            item[\"label\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n            \n        return item\n\nprint(\"[Cell 2 Revised] HierarchicalDeceptionDataset defined with improved error handling.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T23:35:35.229239Z","iopub.execute_input":"2025-04-20T23:35:35.229958Z","iopub.status.idle":"2025-04-20T23:35:35.241023Z","shell.execute_reply.started":"2025-04-20T23:35:35.229927Z","shell.execute_reply":"2025-04-20T23:35:35.240195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% Cell 3: Data Loading and Preprocessing Functions\n\ndef load_processed_data(data_dir=\"processed_data_balanced\"):\n    \"\"\"Load processed data CSVs and print shapes.\"\"\"\n    print(\"[Data] Loading processed CSV data...\")\n    train_df = pd.read_csv(\"/kaggle/input/processed-data/processed_train_balanced.csv\")\n    val_df = pd.read_csv(\"/kaggle/input/processed-data/processed_val.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/processed-data/processed_test.csv\")\n    \n    print(f\"[Data] Train shape: {train_df.shape}\")\n    print(f\"[Data] Validation shape: {val_df.shape}\")\n    print(f\"[Data] Test shape: {test_df.shape}\")\n    \n    # Report original class balance (using 'is_truthful')\n    train_truthful = train_df[\"is_truthful\"].sum()\n    train_deceptive = len(train_df) - train_truthful\n    print(f\"[Data] Original train balance: {train_truthful} truthful, {train_deceptive} deceptive\")\n    print(f\"[Data] Truthful percentage: {train_truthful / len(train_df) * 100:.2f}%\")\n    \n    return train_df, val_df, test_df\n\ndef get_metadata_features(df):\n    \"\"\"Return list of metadata feature names.\"\"\"\n    metadata_features = [\n        \"message_length\", \"word_count\", \"question_count\", \"exclamation_count\",\n        \"has_uncertainty\", \"has_certainty\", \"conversation_length\", \"msg_position_in_convo\", \"position_ratio\"\n    ]\n    # Optionally add extra features if present.\n    if \"sender_is_player\" in df.columns:\n        metadata_features.append(\"sender_is_player\")\n    if \"prev_msg_truthful\" in df.columns:\n        metadata_features.append(\"prev_msg_truthful\")\n    if \"game_stage\" in df.columns:\n        metadata_features.append(\"game_stage\")\n    print(f\"[Data] Metadata features: {metadata_features}\")\n    return metadata_features\n\ndef create_datasets(train_df, val_df, test_df, tokenizer, metadata_features, max_context=5):\n    \"\"\"\n    Create hierarchical datasets.\n    Assumes the CSVs have a column \"cleaned_message\" for current text,\n    a \"context\" column that contains previous messages delimited by \"||\",\n    and an \"is_truthful\" column (which will be inverted to \"is_deceptive\").\n    \"\"\"\n    print(\"[Data] Creating hierarchical datasets...\")\n    # Get texts and context\n    train_texts = train_df[\"cleaned_message\"].fillna(\"\").values\n    val_texts = val_df[\"cleaned_message\"].fillna(\"\").values\n    test_texts = test_df[\"cleaned_message\"].fillna(\"\").values\n    \n    # For context, if not provided, fill with empty strings.\n    if \"context\" in train_df.columns:\n        train_context = train_df[\"context\"].fillna(\"\").values\n        val_context = val_df[\"context\"].fillna(\"\").values\n        test_context = test_df[\"context\"].fillna(\"\").values\n    else:\n        train_context = np.array([\"\"] * len(train_df))\n        val_context = np.array([\"\"] * len(val_df))\n        test_context = np.array([\"\"] * len(test_df))\n    \n    # Invert is_truthful to get is_deceptive label: 1 for deceptive.\n    train_labels = (1 - train_df[\"is_truthful\"]).values\n    val_labels = (1 - val_df[\"is_truthful\"]).values\n    test_labels = (1 - test_df[\"is_truthful\"]).values\n    \n    # Process metadata columns: convert booleans and objects to numerics.\n    for col in metadata_features:\n        for df in [train_df, val_df, test_df]:\n            if col in df.columns:\n                if (df[col].dtype == bool) or (df[col].dtype == \"object\" and df[col].isin([True, False, \"True\", \"False\"]).all()):\n                    df[col] = df[col].map({True: 1, False: 0, \"True\": 1, \"False\": 0})\n                if df[col].dtype == \"object\":\n                    try:\n                        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n                    except Exception as e:\n                        print(f\"[Data] Converting column {col}: error {e}\")\n                        df[col] = df[col].astype(\"category\").cat.codes\n    \n    print(f\"[Data] Using metadata features: {metadata_features}\")\n    train_metadata = train_df[metadata_features].values\n    val_metadata = val_df[metadata_features].values\n    test_metadata = test_df[metadata_features].values\n\n    # Scale metadata\n    metadata_scaler = StandardScaler()\n    train_metadata = metadata_scaler.fit_transform(train_metadata)\n    val_metadata = metadata_scaler.transform(val_metadata)\n    test_metadata = metadata_scaler.transform(test_metadata)\n    joblib.dump(metadata_scaler, \"models/rob_focal_metadata_scaler.pkl\")\n    print(\"[Data] Metadata scaling complete.\")\n    \n    # Create hierarchical datasets\n    train_dataset = HierarchicalDeceptionDataset(train_texts, train_context, train_metadata, train_labels,\n                                                   tokenizer, max_length=64, max_context=max_context)\n    val_dataset = HierarchicalDeceptionDataset(val_texts, val_context, val_metadata, val_labels,\n                                                 tokenizer, max_length=64, max_context=max_context)\n    test_dataset = HierarchicalDeceptionDataset(test_texts, test_context, test_metadata, test_labels,\n                                                  tokenizer, max_length=64, max_context=max_context)\n    print(\"[Data] Hierarchical datasets created.\")\n    return train_dataset, val_dataset, test_dataset\n\nprint(\"[Cell 3] Data loading and preprocessing functions defined.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T23:35:37.655904Z","iopub.execute_input":"2025-04-20T23:35:37.656186Z","iopub.status.idle":"2025-04-20T23:35:37.672374Z","shell.execute_reply.started":"2025-04-20T23:35:37.656166Z","shell.execute_reply":"2025-04-20T23:35:37.671790Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% Cell 4: Training and Evaluation Functions\n\ndef train_model(model, train_dataset, val_dataset, device, batch_size=16, num_epochs=3, learning_rate=2e-5):\n    print(\"[Train] Starting training process...\")\n    \n    # Use focal loss with alpha to emphasize positive (deceptive) examples.\n    focal_loss = FocalLoss(alpha=3.0, gamma=2, reduction=\"mean\")\n    print(\"[Train] Using FocalLoss with alpha=3.0 and gamma=2.\")\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=learning_rate, total_steps=len(train_loader)*num_epochs\n    )\n    \n    best_val_macro_f1 = 0.0\n    patience = 2\n    epochs_no_improve = 0\n    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"val_macro_f1\": []}\n    \n    for epoch in range(num_epochs):\n        model.train()\n        epoch_loss = 0.0\n        correct = 0\n        total = 0\n        print(f\"[Train] Epoch {epoch+1}/{num_epochs} started.\")\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Train\"):\n            # Process current message and context from batch\n            current_ids = batch[\"current_input_ids\"].to(device)\n            current_mask = batch[\"current_attention_mask\"].to(device)\n            context_ids = batch[\"context_input_ids\"].to(device)  # shape [B, max_context, seq_len]\n            context_mask = batch[\"context_attention_mask\"].to(device)\n            metadata = batch[\"metadata\"].to(device)\n            labels = batch[\"label\"].to(device).unsqueeze(1)\n            \n            optimizer.zero_grad()\n            outputs = model(current_ids, current_mask, context_ids, context_mask, metadata)\n            loss = focal_loss(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            \n            epoch_loss += loss.item() * current_ids.size(0)\n            total += current_ids.size(0)\n            preds = (torch.sigmoid(outputs) >= 0.5).float()\n            correct += (preds == labels).sum().item()\n        \n        avg_loss = epoch_loss / total\n        train_acc = correct / total\n        history[\"train_loss\"].append(avg_loss)\n        history[\"train_acc\"].append(train_acc)\n        print(f\"[Train] Epoch {epoch+1} finished. Avg Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n                current_ids = batch[\"current_input_ids\"].to(device)\n                current_mask = batch[\"current_attention_mask\"].to(device)\n                context_ids = batch[\"context_input_ids\"].to(device)\n                context_mask = batch[\"context_attention_mask\"].to(device)\n                metadata = batch[\"metadata\"].to(device)\n                labels = batch[\"label\"].to(device).unsqueeze(1)\n                \n                outputs = model(current_ids, current_mask, context_ids, context_mask, metadata)\n                loss = focal_loss(outputs, labels)\n                val_loss += loss.item() * current_ids.size(0)\n                probs = torch.sigmoid(outputs)\n                preds = (probs >= 0.5).float()\n                all_preds.extend(preds.cpu().numpy().flatten())\n                all_labels.extend(labels.cpu().numpy().flatten())\n        \n        avg_val_loss = val_loss / len(val_dataset)\n        val_acc = accuracy_score(all_labels, all_preds)\n        # Compute macro-F1 using average=\"macro\"\n        val_macro_f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n        history[\"val_loss\"].append(avg_val_loss)\n        history[\"val_acc\"].append(val_acc)\n        history[\"val_macro_f1\"].append(val_macro_f1)\n        print(f\"[Validation] Epoch {epoch+1}: Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}, Macro-F1: {val_macro_f1:.4f}\")\n        \n        if val_macro_f1 > best_val_macro_f1:\n            best_val_macro_f1 = val_macro_f1\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), \"models/hierarchical_model_best_1.pt\")\n            print(f\"[Train] Model improved! Best Macro-F1: {best_val_macro_f1:.4f}\")\n        else:\n            epochs_no_improve += 1\n            print(f\"[Train] No improvement for {epochs_no_improve} epoch(s).\")\n        if epochs_no_improve >= patience:\n            print(\"[Train] Early stopping triggered.\")\n            break\n            \n    # Plot training curves\n    plt.figure(figsize=(12,4))\n    plt.subplot(1,3,1)\n    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss\")\n    plt.legend()\n    plt.subplot(1,3,2)\n    plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n    plt.plot(history[\"val_acc\"], label=\"Val Acc\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Accuracy\")\n    plt.legend()\n    plt.subplot(1,3,3)\n    plt.plot(history[\"val_macro_f1\"], label=\"Val Macro-F1\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.title(\"Macro F1 Score\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(\"results/hierarchical_learning_curves_1.png\")\n    plt.close()\n    \n    model.load_state_dict(torch.load(\"models/hierarchical_model_best_1.pt\"))\n    print(f\"[Train] Loaded best model with Validation Macro-F1: {best_val_macro_f1:.4f}\")\n    return model\n\ndef evaluate_model(model, test_dataset, device, batch_size=16):\n    print(\"[Evaluate] Evaluating on test set...\")\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    model.eval()\n    all_preds, all_labels, all_probs = [], [], []\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n            current_ids = batch[\"current_input_ids\"].to(device)\n            current_mask = batch[\"current_attention_mask\"].to(device)\n            context_ids = batch[\"context_input_ids\"].to(device)\n            context_mask = batch[\"context_attention_mask\"].to(device)\n            metadata = batch[\"metadata\"].to(device)\n            labels = batch[\"label\"].to(device).unsqueeze(1)\n            outputs = model(current_ids, current_mask, context_ids, context_mask, metadata)\n            probs = torch.sigmoid(outputs)\n            preds = (probs >= 0.5).float()\n            all_preds.extend(preds.cpu().numpy().flatten())\n            all_labels.extend(labels.cpu().numpy().flatten())\n            all_probs.extend(probs.cpu().numpy().flatten())\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n    \n    # Compute metrics\n    acc = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, zero_division=0)\n    recall = recall_score(all_labels, all_preds, zero_division=0)\n    f1 = f1_score(all_labels, all_preds, zero_division=0)\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    print(\"[Evaluate] Metrics on test set:\")\n    print(f\"  Accuracy: {acc:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall: {recall:.4f}\")\n    print(f\"  F1 Score: {f1:.4f}\")\n    print(f\"  Macro F1 Score: {macro_f1:.4f}\")\n    print(f\"  Confusion Matrix:\\n{cm}\")\n    \n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n                xticklabels=[\"Truthful\", \"Deceptive\"],\n                yticklabels=[\"Truthful\", \"Deceptive\"])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Test Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(\"results/hierarchical_confusion_matrix_1.png\")\n    plt.close()\n    \n    \n    # Save predictions to CSV\n    df_preds = pd.DataFrame({\"prediction\": all_preds, \"probability\": all_probs, \"true_label\": all_labels})\n    df_preds.to_csv(\"results/hierarchical_predictions_1.csv\", index=False)\n    \n    # Save overall metrics as JSON\n    metrics = {\n        \"accuracy\": acc,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"macro_f1\": macro_f1,\n        \"confusion_matrix\": cm.tolist()\n    }\n    with open(\"results/hierarchical_metrics_1.json\", \"w\") as f:\n        json.dump(metrics, f, indent=4)\n        \n    return metrics\n\nprint(\"[Cell 4] Training and evaluation functions defined.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T23:35:39.311261Z","iopub.execute_input":"2025-04-20T23:35:39.311972Z","iopub.status.idle":"2025-04-20T23:35:39.332900Z","shell.execute_reply.started":"2025-04-20T23:35:39.311947Z","shell.execute_reply":"2025-04-20T23:35:39.332016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% Cell 5: Main Routine\n\ndef main():\n    os.makedirs(\"models\", exist_ok=True)\n    os.makedirs(\"results\", exist_ok=True)\n    \n    print(\"[Main] Loading processed data...\")\n    train_df, val_df, test_df = load_processed_data()\n    \n    # Balance training data based on original is_truthful and then invert label.\n    truthful_df = train_df[train_df[\"is_truthful\"] == 1]\n    deceptive_df = train_df[train_df[\"is_truthful\"] == 0]\n    n_to_sample = len(truthful_df) - len(deceptive_df)\n    print(f\"[Main] Oversampling deceptive class: adding {n_to_sample} samples.\")\n    oversampled_deceptive = deceptive_df.sample(n_to_sample, replace=True, random_state=42)\n    train_df = pd.concat([truthful_df, deceptive_df, oversampled_deceptive])\n    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n    print(f\"[Main] Balanced train data shape: {train_df.shape}\")\n    print(f\"[Main] Validation data shape: {val_df.shape}\")\n    print(f\"[Main] Test data shape: {test_df.shape}\")\n    train_truthful = train_df[\"is_truthful\"].sum()\n    train_deceptive = len(train_df) - train_truthful\n    print(f\"[Main] Original labels: {train_truthful} truthful, {train_deceptive} deceptive\")\n    print(f\"[Main] Truthful percentage: {train_truthful / len(train_df) * 100:.2f}%\")\n    \n    # Initialize tokenizer from RoBERTa\n    print(\"[Main] Initializing tokenizer...\")\n    model_name = \"roberta-base\"\n    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n    print(f\"[Main] Tokenizer loaded from {model_name}\")\n    \n    # Get metadata feature list\n    metadata_features = get_metadata_features(train_df)\n    \n    # Create hierarchical datasets.\n    # (Ensure that your CSVs include a \"context\" column; if not, context will default to empty strings.)\n    train_dataset, val_dataset, test_dataset = create_datasets(train_df, val_df, test_df, tokenizer, metadata_features, max_context=5)\n    \n    # Initialize Hierarchical Model\n    print(\"[Main] Initializing Hierarchical Deception Model...\")\n    model = HierarchicalDeceptionModel(roberta_model_name=model_name, metadata_dim=len(metadata_features), num_context_msgs=5)\n    model = model.to(device)\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"[Main] Total parameters: {total_params:,}\")\n    print(f\"[Main] Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n    \n    # Train model\n    print(\"[Main] Starting training...\")\n    model = train_model(model, train_dataset, val_dataset, device, batch_size=batch_size, num_epochs=5, learning_rate=2e-5)\n    \n    # Evaluate on test set\n    print(\"[Main] Evaluating model on test set...\")\n    test_metrics = evaluate_model(model, test_dataset, device, batch_size=batch_size)\n    print(\"[Main] Test Metrics:\")\n    for k, v in test_metrics.items():\n        print(f\"  {k}: {v}\")\n    \n    # Save final model and tokenizer\n    print(\"[Main] Saving final model and tokenizer...\")\n    torch.save(model.state_dict(), \"models/hierarchical_model_1.pt\")\n    os.makedirs(\"models/hierarchical_tokenizer_1\", exist_ok=True)\n    tokenizer.save_pretrained(\"models/hierarchical_tokenizer_1\")\n    print(\"[Main] Process completed.\")\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T09:43:05.816235Z","iopub.execute_input":"2025-04-21T09:43:05.816477Z","iopub.status.idle":"2025-04-21T09:43:05.821185Z","shell.execute_reply.started":"2025-04-21T09:43:05.816460Z","shell.execute_reply":"2025-04-21T09:43:05.820365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nfrom transformers import RobertaTokenizer\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n\nmetadata_dim = 12  # match the checkpoint\n# model = HierarchicalDeceptionModel(metadata_dim=metadata_dim, max_context=max_context).to(device)\n\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nTOKENIZER_PATH = \"/kaggle/working/models/hierarchical_tokenizer\"\n\nTEST_CSV = \"/kaggle/input/processed-data/processed_test.csv\"  # Adjust this if your test file is differently named\n\nMODEL_PATH    = \"models/hierarchical_model_best_1.pt\"\nSCALER_PATH   = \"models/rob_focal_metadata_scaler.pkl\"\nTEST_CSV      = \"/kaggle/input/processed-data/processed_test.csv\"\nTOKENIZER_NAME= \"roberta-base\"\nBATCH_SIZE    = 16\nMAX_CONTEXT   = 5\n\n# 5) Load model\nprint(\"[Inference] Loading model checkpoint...\")\n# model = HierarchicalDeceptionModel(\n#     # TOKENIZER_NAME=\"roberta-base\",\n#     metadata_dim=len(metadata_features),\n#     max_context=MAX_CONTEXT,\n#     lstm_hidden=256\n# ).to(device)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=device))\nmodel.eval()\n\n# 6) Run inference\nprint(\"[Inference] Running inference...\")\nloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\nall_probs, all_preds = [], []\n\nwith torch.no_grad():\n    for batch in loader:\n        curr_ids   = batch[\"current_input_ids\"].to(device)\n        curr_mask  = batch[\"current_attention_mask\"].to(device)\n        ctx_ids    = batch[\"context_input_ids\"].to(device)\n        ctx_mask   = batch[\"context_attention_mask\"].to(device)\n        metadata   = batch[\"metadata\"].to(device)\n\n        logits, _ = model(curr_ids, curr_mask, ctx_ids, ctx_mask, metadata)\n        probs = torch.sigmoid(logits).squeeze(1)\n        preds = (probs >= 0.5).long()\n\n        all_probs.extend(probs.cpu().tolist())\n        all_preds.extend(preds.cpu().tolist())\n\n# 7) Save predictions\nprint(\"[Inference] Saving predictions...\")\ntest_df[\"predicted_prob\"]  = all_probs\ntest_df[\"predicted_label\"] = all_preds\nos.makedirs(\"results\", exist_ok=True)\nout_csv = \"results/test_predictions.csv\"\ntest_df.to_csv(out_csv, index=False)\nprint(f\"[Inference] Predictions saved to {out_csv}\")\n\n# 8) (Optional) Compute & print test metrics if true labels exist\nif \"is_truthful\" in test_df.columns:\n    y_true = (1 - test_df[\"is_truthful\"].values).astype(int)\n    y_pred = test_df[\"predicted_label\"].values\n    acc      = accuracy_score(y_true, y_pred)\n    prec     = precision_score(y_true, y_pred, zero_division=0)\n    rec      = recall_score(y_true, y_pred, zero_division=0)\n    f1       = f1_score(y_true, y_pred, zero_division=0)\n    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n    cm       = confusion_matrix(y_true, y_pred)\n    print(f\"\\n[Test Metrics] Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f} | Macro-F1: {macro_f1:.4f}\")\n    print(\"Confusion Matrix:\\n\", cm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:36:53.125726Z","iopub.execute_input":"2025-04-21T10:36:53.125985Z","iopub.status.idle":"2025-04-21T10:36:53.155053Z","shell.execute_reply.started":"2025-04-21T10:36:53.125967Z","shell.execute_reply":"2025-04-21T10:36:53.154133Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *Incorporating richer context using hierarchical attention, refining metadata (especially power-dynamic features), experimenting with alternative transformer architectures  and further tuning or augmenting your loss function (e.g., with advanced focal loss or other cost-sensitive methods).*","metadata":{}},{"cell_type":"code","source":"# %% Cell 1: Imports, Setup, and Hyperparameters\nimport os\nimport json\nimport joblib\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaModel, RobertaTokenizer, AutoTokenizer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Device configuration and batch size selection\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"[Setup] Using device: {device}\")\n\nBATCH_SIZE = 16 if device.type == \"cuda\" else 4\nprint(f\"[Setup] Using batch size: {BATCH_SIZE}\")\n\n# Hyperparameters\nMAX_LENGTH = 64           # maximum token length for each message\nMAX_CONTEXT = 5           # maximum number of context messages per sample\nLEARNING_RATE = 2e-5\nNUM_EPOCHS = 5\nFGM_EPSILON = 1.0\nFOCAL_ALPHA = 3.0\nFOCAL_GAMMA = 2\n\n# Transformer model name – you can change to another model if desired.\nTRANSFORMER_MODEL_NAME = \"roberta-base\"\n\n# File paths – adjust as needed\nTRAIN_CSV_PATH = \"/kaggle/input/processed-data/processed_train_balanced.csv\"\nVAL_CSV_PATH   = \"/kaggle/input/processed-data/processed_val.csv\"\nTEST_CSV_PATH  = \"/kaggle/input/processed-data/processed_test.csv\"\n\n# Directories to save models and results\nos.makedirs(\"models\", exist_ok=True)\nos.makedirs(\"results\", exist_ok=True)\nprint(\"[Setup] Hyperparameters and paths are set.\")\n\n# %% Cell 2: Data Loading, Preprocessing, and Balancing\ndef load_data(train_path, val_path, test_path):\n    print(\"[Data] Loading CSV files...\")\n    train_df = pd.read_csv(train_path)\n    val_df   = pd.read_csv(val_path)\n    test_df  = pd.read_csv(test_path)\n    print(f\"[Data] Train shape: {train_df.shape}, Val shape: {val_df.shape}, Test shape: {test_df.shape}\")\n    return train_df, val_df, test_df\n\ndef get_metadata_feature_names(df):\n    # Define the list of metadata features (adjust as needed)\n    metadata_features = [\n        \"message_length\", \"word_count\", \"question_count\", \"exclamation_count\",\n        \"has_uncertainty\", \"has_certainty\", \"conversation_length\", \"msg_position_in_convo\", \"position_ratio\"\n    ]\n    for col in [\"sender_is_player\", \"prev_msg_truthful\", \"game_stage\"]:\n        if col in df.columns:\n            metadata_features.append(col)\n    print(f\"[Data] Using metadata features: {metadata_features}\")\n    return metadata_features\n\ndef preprocess_data(df, metadata_features):\n    # Ensure text columns are strings\n    df[\"cleaned_message\"] = df[\"cleaned_message\"].fillna(\"\")\n    if \"context\" in df.columns:\n        df[\"context\"] = df[\"context\"].fillna(\"\")\n    else:\n        df[\"context\"] = \"\"\n    \n    # Convert metadata columns to numeric if necessary\n    for col in metadata_features:\n        if col in df.columns and df[col].dtype == object:\n            try:\n                df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n            except Exception as e:\n                print(f\"[Data] Conversion error for {col}: {e}\")\n                df[col] = df[col].astype(\"category\").cat.codes\n    return df\n\ndef process_labels(df):\n    # Invert 'is_truthful' to create 'is_deceptive': 1 means deceptive, 0 means truthful.\n    if \"is_truthful\" in df.columns:\n        df[\"is_deceptive\"] = 1 - df[\"is_truthful\"]\n    else:\n        df[\"is_deceptive\"] = -1  # placeholder if missing\n    return df\n\ndef balance_training_data(train_df):\n    # Balance the training data via oversampling if required.\n    truthful_df = train_df[train_df[\"is_truthful\"] == 1]\n    deceptive_df = train_df[train_df[\"is_truthful\"] == 0]\n    n_to_sample = len(truthful_df) - len(deceptive_df)\n    \n    if n_to_sample > 0:\n        print(f\"[Balance] Oversampling deceptive class by {n_to_sample} samples...\")\n        oversampled_deceptive = deceptive_df.sample(n_to_sample, replace=True, random_state=42)\n        train_df = pd.concat([truthful_df, deceptive_df, oversampled_deceptive])\n    else:\n        print(\"[Balance] No oversampling needed.\")\n    \n    return train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\ndef scale_metadata(train_df, val_df, test_df, metadata_features):\n    # Drop rows with missing metadata and scale the metadata features.\n    train_df = train_df.dropna(subset=metadata_features)\n    val_df = val_df.dropna(subset=metadata_features)\n    test_df = test_df.dropna(subset=metadata_features)\n    \n    for df in [train_df, val_df, test_df]:\n        assert set(metadata_features).issubset(df.columns), \"Some metadata features are missing.\"\n    \n    scaler = StandardScaler()\n    train_meta = scaler.fit_transform(train_df[metadata_features].values)\n    val_meta   = scaler.transform(val_df[metadata_features].values)\n    test_meta  = scaler.transform(test_df[metadata_features].values)\n    \n    joblib.dump(scaler, \"models/metadata_scaler_2.pkl\")\n    print(\"[Data] Metadata scaler saved to models/metadata_scaler_2.pkl\")\n    \n    return train_meta, val_meta, test_meta\n\n# %% Cell 3: Model and Dataset Definitions\n# Hierarchical Attention Module\nclass AttentionLayer(nn.Module):\n    def __init__(self, input_dim):\n        super(AttentionLayer, self).__init__()\n        self.attn = nn.Linear(input_dim, 1)\n    \n    def forward(self, inputs):\n        # inputs: [batch, num_context, hidden_dim]\n        attn_scores = self.attn(inputs)  # [batch, num_context, 1]\n        attn_weights = torch.softmax(attn_scores, dim=1)\n        context_vector = torch.sum(attn_weights * inputs, dim=1)  # [batch, hidden_dim]\n        return context_vector, attn_weights\n\n# Final Hierarchical Attention Deception Model\nclass HierarchicalAttentionDeceptionModel(nn.Module):\n    def __init__(self, transformer_model_name=TRANSFORMER_MODEL_NAME, metadata_dim=9, max_context=5, lstm_hidden=256):\n        super(HierarchicalAttentionDeceptionModel, self).__init__()\n        print(\"[Model] Loading transformer model...\")\n        self.transformer = RobertaModel.from_pretrained(transformer_model_name)\n        self.transformer_hidden = self.transformer.config.hidden_size  #  768 for roberta-base\n        self.max_context = max_context\n        \n        # LSTM for context representations\n        self.context_lstm = nn.LSTM(input_size=self.transformer_hidden, hidden_size=lstm_hidden,\n                                    batch_first=True, bidirectional=True)\n        self.lstm_output_dim = lstm_hidden * 2\n        \n        # Hierarchical Attention Layer\n        self.attn_layer = AttentionLayer(self.lstm_output_dim)\n        \n        # Metadata branch\n        self.metadata_fc = nn.Sequential(\n            nn.Linear(metadata_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        \n        # Fusion layer: current message + context vector + metadata\n        fusion_dim = self.transformer_hidden + self.lstm_output_dim + 64\n        self.fusion_fc = nn.Sequential(\n            nn.Linear(fusion_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 1)\n        )\n        print(\"[Model] HierarchicalAttentionDeceptionModel initialized.\")\n\n    def forward(self, current_input_ids, current_attention_mask, context_input_ids, context_attention_mask, metadata):\n        # Encode current message\n        current_outputs = self.transformer(input_ids=current_input_ids, attention_mask=current_attention_mask)\n        current_repr = current_outputs.last_hidden_state[:, 0, :]  # [batch, hidden]\n        \n        # Encode context messages\n        batch_size, num_ctx, seq_len = context_input_ids.shape\n        flat_ctx_ids = context_input_ids.view(-1, seq_len)\n        flat_ctx_mask = context_attention_mask.view(-1, seq_len)\n        context_outputs = self.transformer(input_ids=flat_ctx_ids, attention_mask=flat_ctx_mask)\n        ctx_embeddings = context_outputs.last_hidden_state[:, 0, :]  # [batch*num_ctx, hidden]\n        ctx_embeddings = ctx_embeddings.view(batch_size, num_ctx, self.transformer_hidden)\n        \n        # LSTM over context embeddings and attention\n        lstm_out, _ = self.context_lstm(ctx_embeddings)\n        context_vector, attn_weights = self.attn_layer(lstm_out)\n        \n        # Process metadata\n        meta_out = self.metadata_fc(metadata)\n        \n        # Fusion of current message, context vector, and metadata\n        fusion_input = torch.cat((current_repr, context_vector, meta_out), dim=1)\n        output = self.fusion_fc(fusion_input)\n        return output, attn_weights\n\n# Dataset Definition\nclass HierarchicalDeceptionDataset(Dataset):\n    def __init__(self, texts, context_texts, metadata, labels=None, tokenizer=None, \n                 max_length=64, max_context=5, context_delim=\"||\"):\n        \"\"\"\n        texts: list of current message strings.\n        context_texts: list of context strings (each composed of messages separated by context_delim).\n        metadata: numpy array of metadata features (rows must match texts length).\n        labels: list of target labels (1 for deceptive, 0 for truthful)\n        tokenizer: Hugging Face tokenizer.\n        \"\"\"\n        self.texts = texts\n        self.context_texts = context_texts\n        self.metadata = metadata\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.max_context = max_context\n        self.context_delim = context_delim\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        # Process current message\n        curr_text = self.texts[idx]\n        curr_text = \"\" if curr_text is None or (isinstance(curr_text, float) and np.isnan(curr_text)) else str(curr_text)\n        curr_enc = self.tokenizer(\n            curr_text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        current_input_ids = curr_enc[\"input_ids\"].squeeze(0)\n        current_attention_mask = curr_enc[\"attention_mask\"].squeeze(0)\n        \n        # Process context messages\n        ctx_entry = self.context_texts[idx]\n        ctx_entry = \"\" if ctx_entry is None or (isinstance(ctx_entry, float) and np.isnan(ctx_entry)) else str(ctx_entry)\n        context_msgs = [msg.strip() for msg in ctx_entry.split(self.context_delim) if msg.strip()]\n        context_msgs = context_msgs[-self.max_context:]\n        while len(context_msgs) < self.max_context:\n            context_msgs.insert(0, \"\")\n        context_encodings = [self.tokenizer(\n            msg,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        ) for msg in context_msgs]\n        context_input_ids = torch.stack([enc[\"input_ids\"].squeeze(0) for enc in context_encodings])\n        context_attention_mask = torch.stack([enc[\"attention_mask\"].squeeze(0) for enc in context_encodings])\n        \n        # Process metadata (using same index from the scaled metadata array)\n        meta = torch.tensor(self.metadata[idx], dtype=torch.float)\n        \n        item = {\n            \"current_input_ids\": current_input_ids,\n            \"current_attention_mask\": current_attention_mask,\n            \"context_input_ids\": context_input_ids,\n            \"context_attention_mask\": context_attention_mask,\n            \"metadata\": meta\n        }\n        if self.labels is not None:\n            item[\"label\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n        return item\n\nprint(\"[Cell 3] Model and dataset definitions loaded.\")\n\n#  Adversarial Training (FGM) and Focal Loss\nclass FGM:\n    def __init__(self, model, epsilon=FGM_EPSILON, emb_name=\"embeddings.word_embeddings.weight\"):\n        self.model = model\n        self.epsilon = epsilon\n        self.emb_name = emb_name\n        self.backup = {}\n\n    def attack(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and self.emb_name in name:\n                self.backup[name] = param.data.clone()\n                norm = torch.norm(param.grad)\n                if norm != 0:\n                    r_at = self.epsilon * param.grad / norm\n                    param.data.add_(r_at)\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and self.emb_name in name and name in self.backup:\n                param.data = self.backup[name]\n        self.backup = {}\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA, reduction=\"mean\"):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        \n    def forward(self, inputs, targets):\n        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n        probs = torch.sigmoid(inputs)\n        p_t = targets * probs + (1 - targets) * (1 - probs)\n        loss = self.alpha * (1 - p_t) ** self.gamma * bce_loss\n        if self.reduction == \"mean\":\n            return loss.mean()\n        elif self.reduction == \"sum\":\n            return loss.sum()\n        return loss\n\nprint(\"[Cell 4] FGM and FocalLoss defined.\")\n\n# %% Cell 5: Training, Evaluation, and Dataset Creation Functions\ndef train_model(model, train_dataset, val_dataset, device, batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE):\n    print(\"[Train] Starting training process...\")\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA, reduction=\"mean\")\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n                                                     total_steps=len(train_loader)*num_epochs)\n    fgm = FGM(model, epsilon=FGM_EPSILON)\n    \n    best_val_macro_f1 = 0.0\n    patience = 2\n    epochs_no_improve = 0\n    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"val_macro_f1\": []}\n    \n    for epoch in range(num_epochs):\n        checkpoint_path = f\"model_checkpoint_epoch_{epoch+1}.pt\"\n        torch.save(model.state_dict(), checkpoint_path)\n        print(f\"Model checkpoint saved at {checkpoint_path}\")\n        model.train()\n        total_loss = 0.0\n        correct = 0\n        total = 0\n        print(f\"[Train] Epoch {epoch+1}/{num_epochs} started.\")\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Train\"):\n            current_ids = batch[\"current_input_ids\"].to(device)\n            current_mask = batch[\"current_attention_mask\"].to(device)\n            context_ids = batch[\"context_input_ids\"].to(device)\n            context_mask = batch[\"context_attention_mask\"].to(device)\n            metadata = batch[\"metadata\"].to(device)\n            labels = batch[\"label\"].to(device).unsqueeze(1)\n            \n            optimizer.zero_grad()\n            outputs, _ = model(current_ids, current_mask, context_ids, context_mask, metadata)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            \n            # FGM adversarial attack\n            fgm.attack()\n            outputs_adv, _ = model(current_ids, current_mask, context_ids, context_mask, metadata)\n            loss_adv = criterion(outputs_adv, labels)\n            loss_adv.backward()\n            fgm.restore()\n            \n            optimizer.step()\n            scheduler.step()\n            \n            total_loss += loss.item() * current_ids.size(0)\n            total += current_ids.size(0)\n            preds = (torch.sigmoid(outputs) >= 0.5).float()\n            correct += (preds == labels).sum().item()\n        \n        avg_loss = total_loss / total\n        train_acc = correct / total\n        history[\"train_loss\"].append(avg_loss)\n        history[\"train_acc\"].append(train_acc)\n        print(f\"[Train] Epoch {epoch+1} finished. Loss: {avg_loss:.4f}, Acc: {train_acc:.4f}\")\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n                current_ids = batch[\"current_input_ids\"].to(device)\n                current_mask = batch[\"current_attention_mask\"].to(device)\n                context_ids = batch[\"context_input_ids\"].to(device)\n                context_mask = batch[\"context_attention_mask\"].to(device)\n                metadata = batch[\"metadata\"].to(device)\n                labels = batch[\"label\"].to(device).unsqueeze(1)\n                outputs, _ = model(current_ids, current_mask, context_ids, context_mask, metadata)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * current_ids.size(0)\n                preds = (torch.sigmoid(outputs) >= 0.5).float()\n                all_preds.extend(preds.cpu().numpy().flatten())\n                all_labels.extend(labels.cpu().numpy().flatten())\n        avg_val_loss = val_loss / len(val_dataset)\n        val_acc = accuracy_score(all_labels, all_preds)\n        val_macro_f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n        history[\"val_loss\"].append(avg_val_loss)\n        history[\"val_acc\"].append(val_acc)\n        history[\"val_macro_f1\"].append(val_macro_f1)\n        print(f\"[Validation] Epoch {epoch+1}: Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}, Macro-F1: {val_macro_f1:.4f}\")\n        \n        if val_macro_f1 > best_val_macro_f1:\n            best_val_macro_f1 = val_macro_f1\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), \"models/best_hierarchical_model_2.pt\")\n            print(f\"[Train] Model improved. Saving model with Macro-F1: {best_val_macro_f1:.4f}\")\n        else:\n            epochs_no_improve += 1\n            print(f\"[Train] No improvement for {epochs_no_improve} epoch(s).\")\n        if epochs_no_improve >= patience:\n            print(\"[Train] Early stopping triggered.\")\n            break\n\n    # Plot training curves\n    plt.figure(figsize=(12,4))\n    plt.subplot(1,3,1)\n    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.subplot(1,3,2)\n    plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n    plt.plot(history[\"val_acc\"], label=\"Val Acc\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.subplot(1,3,3)\n    plt.plot(history[\"val_macro_f1\"], label=\"Val Macro-F1\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Macro-F1\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(\"results/training_curves.png\")\n    plt.close()\n    \n    # Load best model for evaluation\n    model.load_state_dict(torch.load(\"models/best_hierarchical_model_2.pt\"))\n    print(f\"[Train] Loaded best model with Macro-F1: {best_val_macro_f1:.4f}\")\n    return model\n\ndef evaluate_model(model, test_dataset, device, batch_size=BATCH_SIZE):\n    print(\"[Evaluate] Evaluating on test set...\")\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    model.eval()\n    all_preds, all_labels, all_probs = [], [], []\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n            current_ids = batch[\"current_input_ids\"].to(device)\n            current_mask = batch[\"current_attention_mask\"].to(device)\n            context_ids = batch[\"context_input_ids\"].to(device)\n            context_mask = batch[\"context_attention_mask\"].to(device)\n            metadata = batch[\"metadata\"].to(device)\n            labels = batch[\"label\"].to(device).unsqueeze(1)\n            outputs, _ = model(current_ids, current_mask, context_ids, context_mask, metadata)\n            probs = torch.sigmoid(outputs)\n            preds = (probs >= 0.5).float()\n            all_preds.extend(preds.cpu().numpy().flatten())\n            all_labels.extend(labels.cpu().numpy().flatten())\n            all_probs.extend(probs.cpu().numpy().flatten())\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n    \n    acc = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, zero_division=0)\n    recall = recall_score(all_labels, all_preds, zero_division=0)\n    f1 = f1_score(all_labels, all_preds, zero_division=0)\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    print(\"[Evaluate] Metrics on test set:\")\n    print(f\"  Accuracy: {acc:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall: {recall:.4f}\")\n    print(f\"  F1 Score: {f1:.4f}\")\n    print(f\"  Macro F1 Score: {macro_f1:.4f}\")\n    print(f\"  Confusion Matrix:\\n{cm}\")\n    \n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n                xticklabels=[\"Truthful\", \"Deceptive\"],\n                yticklabels=[\"Truthful\", \"Deceptive\"])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Test Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(\"results/hierarchical_confusion_matrix_2.png\")\n    plt.close()\n    \n    # Save predictions and metrics\n    df_preds = pd.DataFrame({\"prediction\": all_preds, \"probability\": all_probs, \"true_label\": all_labels})\n    df_preds.to_csv(\"results/hierarchical_predictions_2.csv\", index=False)\n    \n    metrics = {\n        \"accuracy\": acc,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"macro_f1\": macro_f1,\n        \"confusion_matrix\": cm.tolist()\n    }\n    with open(\"results/hierarchical_metrics_2.json\", \"w\") as f:\n        json.dump(metrics, f, indent=4)\n        \n    return metrics\n\ndef create_datasets(train_df, val_df, test_df, tokenizer, metadata_features):\n    # Scale metadata AFTER all data modifications (like balancing) are performed.\n    train_meta, val_meta, test_meta = scale_metadata(train_df, val_df, test_df, metadata_features)\n    train_dataset = HierarchicalDeceptionDataset(\n        texts=train_df[\"cleaned_message\"].tolist(), \n        context_texts=train_df[\"context\"].tolist(), \n        metadata=train_meta,\n        labels=train_df[\"is_deceptive\"].tolist(),\n        tokenizer=tokenizer,\n        max_length=MAX_LENGTH,\n        max_context=MAX_CONTEXT\n    )\n    val_dataset = HierarchicalDeceptionDataset(\n        texts=val_df[\"cleaned_message\"].tolist(), \n        context_texts=val_df[\"context\"].tolist(), \n        metadata=val_meta,\n        labels=val_df[\"is_deceptive\"].tolist(),\n        tokenizer=tokenizer,\n        max_length=MAX_LENGTH,\n        max_context=MAX_CONTEXT\n    )\n    test_dataset = HierarchicalDeceptionDataset(\n        texts=test_df[\"cleaned_message\"].tolist(), \n        context_texts=test_df[\"context\"].tolist(), \n        metadata=test_meta,\n        labels=test_df[\"is_deceptive\"].tolist(),\n        tokenizer=tokenizer,\n        max_length=MAX_LENGTH,\n        max_context=MAX_CONTEXT\n    )\n    return train_dataset, val_dataset, test_dataset\n\ndef initialize_and_train_model(train_dataset, val_dataset, metadata_features, tokenizer):\n    print(\"[Init] Initializing Hierarchical Deception Model...\")\n    model = HierarchicalAttentionDeceptionModel(\n        transformer_model_name=TRANSFORMER_MODEL_NAME,\n        metadata_dim=len(metadata_features),\n        max_context=MAX_CONTEXT,\n        lstm_hidden=256\n    ).to(device)\n\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"[Init] Model Parameters: {total_params:,} total / {trainable_params:,} trainable\")\n\n    model = train_model(model, train_dataset, val_dataset, device,\n                        batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE)\n    return model\n\ndef evaluate_and_save(model, test_dataset, tokenizer, model_path, tokenizer_path):\n    print(\"[Eval] Evaluating model on test set...\")\n    test_metrics = evaluate_model(model, test_dataset, device, batch_size=BATCH_SIZE)\n    print(\"[Eval] Test Metrics:\")\n    for k, v in test_metrics.items():\n        # For scalar metric values\n        if isinstance(v, float):\n            print(f\"  {k}: {v:.4f}\")\n        else:\n            print(f\"  {k}: {v}\")\n    print(\"[Save] Saving model and tokenizer...\")\n    torch.save(model.state_dict(), model_path)\n    os.makedirs(tokenizer_path, exist_ok=True)\n    tokenizer.save_pretrained(tokenizer_path)\n    print(\"[Save] Model and tokenizer saved successfully.\")\n\n# %% Cell 6: Main Execution Pipeline\ndef main():\n    print(\"[Main] Loading data...\")\n    train_df, val_df, test_df = load_data(TRAIN_CSV_PATH, VAL_CSV_PATH, TEST_CSV_PATH)\n    \n    # Preprocess and set up the data.\n    metadata_features = get_metadata_feature_names(train_df)\n    train_df = preprocess_data(train_df, metadata_features)\n    val_df   = preprocess_data(val_df, metadata_features)\n    test_df  = preprocess_data(test_df, metadata_features)\n    train_df = process_labels(train_df)\n    val_df   = process_labels(val_df)\n    test_df  = process_labels(test_df)\n    \n    print(\"[Main] Balancing training data...\")\n    train_df = balance_training_data(train_df)\n    \n    print(\"[Main] Initializing tokenizer...\")\n    tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n    print(f\"[Main] Tokenizer loaded: {TRANSFORMER_MODEL_NAME}\")\n    \n    print(\"[Main] Creating datasets...\")\n    train_dataset, val_dataset, test_dataset = create_datasets(train_df, val_df, test_df, tokenizer, metadata_features)\n    print(f\"[Main] Dataset sizes — Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n    \n    model = initialize_and_train_model(train_dataset, val_dataset, metadata_features, tokenizer)\n    \n    evaluate_and_save(\n        model,\n        test_dataset,\n        tokenizer,\n        model_path=\"models/hierarchical_model_2.pt\",\n        tokenizer_path=\"models/hierarchical_tokenizer_2\"\n    )\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T03:03:17.687276Z","iopub.execute_input":"2025-04-21T03:03:17.687788Z","iopub.status.idle":"2025-04-21T05:33:58.624423Z","shell.execute_reply.started":"2025-04-21T03:03:17.687766Z","shell.execute_reply":"2025-04-21T05:33:58.623739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% Cell X: Inference and Evaluation\nimport torch\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\nimport joblib\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport json\n\n# Constants and paths (reuse those from your notebook)\nSCALER_PATH = \"models/metadata_scaler_2.pkl\"\nMODEL_PATH = \"models/best_hierarchical_model_2.pt\"\nTEST_CSV_PATH = \"/kaggle/input/processed-data/processed_test.csv\"\n\n# Reuse previously defined variables and classes in this notebook environment:\n# TRANSFORMER_MODEL_NAME, MAX_LENGTH, MAX_CONTEXT, BATCH_SIZE, device\n# HierarchicalAttentionDeceptionModel, HierarchicalDeceptionDataset\n# Also reuse preprocessing functions: process_labels, get_metadata_feature_names\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n\n# Load and preprocess test data\ndf_test = pd.read_csv(TEST_CSV_PATH)\nif \"is_truthful\" in df_test.columns:\n    df_test[\"is_deceptive\"] = 1 - df_test[\"is_truthful\"]\n# Derive metadata feature names\ndef get_metadata_feature_names(df):\n    metadata_features = [\n        \"message_length\", \"word_count\", \"question_count\", \"exclamation_count\",\n        \"has_uncertainty\", \"has_certainty\", \"conversation_length\", \"msg_position_in_convo\", \"position_ratio\"\n    ]\n    for col in [\"sender_is_player\", \"prev_msg_truthful\", \"game_stage\"]:\n        if col in df.columns:\n            metadata_features.append(col)\n    return metadata_features\nmetadata_features = get_metadata_feature_names(df_test)\n# Fill missing metadata\nfor col in metadata_features:\n    if col not in df_test:\n        df_test[col] = 0\n    df_test[col] = pd.to_numeric(df_test[col], errors='coerce').fillna(0)\n\n# Scale metadata features\nscaler = joblib.load(SCALER_PATH)\nscaled_meta = scaler.transform(df_test[metadata_features].values)\n\n# Create test dataset (labels available for evaluation)\ntest_dataset = HierarchicalDeceptionDataset(\n    texts=df_test[\"cleaned_message\"].fillna(\"\").tolist(),\n    context_texts=df_test.get(\"context\", pd.Series([\"\"]*len(df_test))).fillna(\"\").tolist(),\n    metadata=scaled_meta,\n    labels=df_test[\"is_deceptive\"].tolist(),\n    tokenizer=tokenizer,\n    max_length=MAX_LENGTH,\n    max_context=MAX_CONTEXT\n)\n\n# Initialize model and load weights\nmodel = HierarchicalAttentionDeceptionModel(\n    tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n,\n    metadata_dim=len(metadata_features),\n    max_context=MAX_CONTEXT,\n    lstm_hidden=256\n).to(device)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=device))\nmodel.eval()\n\n# Run inference and collect predictions\nloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\nall_preds, all_probs, all_labels = [], [], []\nwith torch.no_grad():\n    for batch in loader:\n        curr_ids = batch[\"current_input_ids\"].to(device)\n        curr_mask = batch[\"current_attention_mask\"].to(device)\n        ctx_ids = batch[\"context_input_ids\"].to(device)\n        ctx_mask = batch[\"context_attention_mask\"].to(device)\n        meta = batch[\"metadata\"].to(device)\n        labels = batch[\"label\"].to(device)\n\n        outputs, _ = model(curr_ids, curr_mask, ctx_ids, ctx_mask, meta)\n        probs = torch.sigmoid(outputs).squeeze(1)\n        preds = (probs >= 0.5).long()\n\n        all_probs.extend(probs.cpu().numpy())\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Evaluate metrics\nacc = accuracy_score(all_labels, all_preds)\nprec = precision_score(all_labels, all_preds, zero_division=0)\nrec = recall_score(all_labels, all_preds, zero_division=0)\nf1 = f1_score(all_labels, all_preds, zero_division=0)\nmacro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\ncm = confusion_matrix(all_labels, all_preds)\n\n# Print metrics\nprint(\"Evaluation Metrics on Test Set:\")\nprint(f\"  Accuracy: {acc:.4f}\")\nprint(f\"  Precision: {prec:.4f}\")\nprint(f\"  Recall: {rec:.4f}\")\nprint(f\"  F1 Score: {f1:.4f}\")\nprint(f\"  Macro F1: {macro_f1:.4f}\")\nprint(\"  Confusion Matrix:\")\nprint(cm)\n\n# Save predictions and metrics\ndf_test[\"predicted_label\"] = all_preds\ndf_test[\"predicted_prob\"] = all_probs\noutput_pred_path = \"results/test_predictions.csv\"\ndf_test.to_csv(output_pred_path, index=False)\nmetrics = {\n    \"accuracy\": acc,\n    \"precision\": prec,\n    \"recall\": rec,\n    \"f1\": f1,\n    \"macro_f1\": macro_f1,\n    \"confusion_matrix\": cm.tolist()\n}\nwith open(\"results/test_metrics.json\", \"w\") as f:\n    json.dump(metrics, f, indent=4)\n# Also print the saved metrics dictionary\nprint(\"Saved metrics:\")\nprint(json.dumps(metrics, indent=4))\n\nprint(f\"Inference and evaluation complete. Predictions saved to {output_pred_path} and metrics to results/test_metrics.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:50:18.996086Z","iopub.execute_input":"2025-04-21T10:50:18.996765Z","iopub.status.idle":"2025-04-21T10:50:19.023328Z","shell.execute_reply.started":"2025-04-21T10:50:18.996742Z","shell.execute_reply":"2025-04-21T10:50:19.022541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # %% Cell 7: Interactive Inference for a Single Example\n# def interactive_inference(model, tokenizer, sample_text, sample_context, sample_metadata):\n#     model.eval()\n#     enc_current = tokenizer(\n#         sample_text,\n#         add_special_tokens=True,\n#         max_length=MAX_LENGTH,\n#         padding=\"max_length\",\n#         truncation=True,\n#         return_tensors=\"pt\"\n#     )\n#     current_ids = enc_current[\"input_ids\"].to(device)\n#     current_mask = enc_current[\"attention_mask\"].to(device)\n    \n#     context_msgs = [msg.strip() for msg in sample_context.split(\"||\") if msg.strip()]\n#     context_msgs = context_msgs[-MAX_CONTEXT:]\n#     while len(context_msgs) < MAX_CONTEXT:\n#         context_msgs.insert(0, \"\")\n#     context_encodings = [tokenizer(\n#         msg,\n#         add_special_tokens=True,\n#         max_length=MAX_LENGTH,\n#         padding=\"max_length\",\n#         truncation=True,\n#         return_tensors=\"pt\"\n#     ) for msg in context_msgs]\n#     context_ids = torch.stack([enc[\"input_ids\"].squeeze(0) for enc in context_encodings]).unsqueeze(0).to(device)\n#     context_mask = torch.stack([enc[\"attention_mask\"].squeeze(0) for enc in context_encodings]).unsqueeze(0).to(device)\n    \n#     meta = torch.tensor(sample_metadata, dtype=torch.float).unsqueeze(0).to(device)\n    \n#     with torch.no_grad():\n#         output, attn_weights = model(current_ids, current_mask, context_ids, context_mask, meta)\n#         prob = torch.sigmoid(output).item()\n#         pred = int(prob >= 0.5)\n#     print(f\"[Interactive] Prediction: {pred} (Probability: {prob:.4f})\")\n#     print(f\"[Interactive] Attention Weights on Context: {attn_weights.squeeze(0).cpu().numpy()}\")\n#     return pred, prob, attn_weights\n\n# # Example usage:\n# sample_text = \"I believe your proposal has merit, but we must be cautious.\"\n# sample_context = \"Let's discuss our options.||Your previous message was insightful.\"\n# sample_metadata = test_meta[0]  # using first sample's metadata as an example\n# interactive_inference(model, tokenizer, sample_text, sample_context, sample_metadata)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T00:54:01.300296Z","iopub.status.idle":"2025-04-21T00:54:01.300616Z","shell.execute_reply.started":"2025-04-21T00:54:01.300462Z","shell.execute_reply":"2025-04-21T00:54:01.300476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Another try of above method implementation**","metadata":{}},{"cell_type":"markdown","source":"• Switched to the AdamW optimizer with weight decay and introduced a learning rate scheduler.\n\n• Added a placeholder for text data augmentation \n\n• Updated the metadata branch by including a simple attention mechanism (a “MetadataAttention”  to allow the network to weigh metadata features dynamically.\n\n• Made minor adjustments in dropout rates and hyperparameters.","metadata":{}},{"cell_type":"code","source":"import os\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW  \nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom transformers import RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\nprint(\"[Setup] All packages imported.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:53:46.797686Z","iopub.execute_input":"2025-04-21T10:53:46.797958Z","iopub.status.idle":"2025-04-21T10:53:46.803100Z","shell.execute_reply.started":"2025-04-21T10:53:46.797938Z","shell.execute_reply":"2025-04-21T10:53:46.802587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import random\n\ndef augment_text(text, aug_prob=0.3):\n    \"\"\"\n    A simple placeholder for text augmentation.\n    Currently, it randomly lowercases some words as a dummy augmentation.\n    Replace or extend this function with your preferred augmentation technique.\n    \"\"\"\n    words = text.split()\n    new_words = [word.lower() if random.random() < aug_prob else word for word in words]\n    return \" \".join(new_words)\n\n# # Example usage:\n# sample_text = \"This is an Example of a Diplomatic message.\"\n# augmented_text = augment_text(sample_text)\n# print(\"[Augmentation] Original:\", sample_text)\n# print(\"[Augmentation] Augmented:\", augmented_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:53:47.242576Z","iopub.execute_input":"2025-04-21T10:53:47.243052Z","iopub.status.idle":"2025-04-21T10:53:47.247121Z","shell.execute_reply.started":"2025-04-21T10:53:47.243033Z","shell.execute_reply":"2025-04-21T10:53:47.246428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def engineer_metadata(df):\n    df = df.copy()\n    df['score_delta'] = df['game_score'] - df['game_score_delta']\n    df['is_sender_leading'] = (df['game_score'] > df['game_score'].mean()).astype(int)\n    df['punctuation_density'] = (df['exclamation_count'] + df['question_count']) / (df['message_length'] + 1e-5)\n    df['score_ratio'] = df['game_score'] / (df['game_score_delta'] + 1e-5)\n    print(\"[FeatureEng] Added score_delta, is_sender_leading, punctuation_density, score_ratio.\")\n    return df\n\n# Load your CSV files and apply metadata engineering:\ntrain_df = pd.read_csv(\"/kaggle/input/processed-data/processed_train_balanced.csv\")\nval_df   = pd.read_csv(\"/kaggle/input/processed-data/processed_val.csv\")\ntest_df  = pd.read_csv(\"/kaggle/input/processed-data/processed_test.csv\")\n\nprint(f\"[Data] Train shape: {train_df.shape}\")\nprint(f\"[Data] Validation shape: {val_df.shape}\")\nprint(f\"[Data] Test shape: {test_df.shape}\")\n\ntrain_df = engineer_metadata(train_df)\nval_df = engineer_metadata(val_df)\ntest_df = engineer_metadata(test_df)\n\n# List of metadata features (update if needed):\nmetadata_features = ['message_length', 'word_count', 'question_count', 'exclamation_count',\n                     'has_uncertainty', 'has_certainty', 'conversation_length',\n                     'msg_position_in_convo', 'position_ratio', 'sender_is_player',\n                     'prev_msg_truthful', 'game_stage']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:53:49.563564Z","iopub.execute_input":"2025-04-21T10:53:49.563836Z","iopub.status.idle":"2025-04-21T10:53:49.780469Z","shell.execute_reply.started":"2025-04-21T10:53:49.563814Z","shell.execute_reply":"2025-04-21T10:53:49.779721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"[Data] Initializing tokenizer...\")\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nprint(\"[Data] Tokenizer loaded from roberta-base\")\n\nclass HierarchicalDataset(Dataset):\n    def __init__(self, df, tokenizer, metadata_features, max_length=64, max_context=5, augment=False):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.metadata_features = metadata_features\n        self.max_length = max_length\n        self.max_context = max_context\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        # Use data augmentation on the message if desired.\n        message = row['message']\n        if self.augment:\n            message = augment_text(message)\n\n        # Use context if available; otherwise, duplicate the message as dummy context.\n        context = row.get('context', \"\")\n        context_list = [context if context != \"\" else message for _ in range(self.max_context)]\n        \n        # Tokenize current message.\n        enc = self.tokenizer(message,\n                             padding='max_length',\n                             truncation=True,\n                             max_length=self.max_length,\n                             return_tensors=\"pt\")\n        # Tokenize context messages.\n        context_encodings = [self.tokenizer(ctx,\n                                            padding='max_length',\n                                            truncation=True,\n                                            max_length=self.max_length,\n                                            return_tensors=\"pt\")\n                             for ctx in context_list]\n        ctx_input_ids = torch.stack([ce[\"input_ids\"].squeeze() for ce in context_encodings])\n        ctx_attention_mask = torch.stack([ce[\"attention_mask\"].squeeze() for ce in context_encodings])\n        \n        # Metadata vector.\n        meta = row[self.metadata_features].apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(np.float32)\n\n        meta = torch.tensor(meta)\n        \n        # Label, assuming binary classification in column 'label'.\n        # label = torch.tensor([row['label']], dtype=torch.float32)\n        label = torch.tensor([row['is_truthful']], dtype=torch.float32)\n        \n        sample = {\n            \"input_ids\": enc[\"input_ids\"].squeeze(),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n            \"context_input_ids\": ctx_input_ids,\n            \"context_attention_mask\": ctx_attention_mask,\n            \"metadata\": meta\n        }\n        return sample, label\n\n# Create datasets (set augment=True for training if you wish to augment your data).\nmax_length = 64\nmax_context = 5\nbatch_size = 8\n\ntrain_dataset = HierarchicalDataset(train_df, tokenizer, metadata_features, max_length, max_context, augment=True)\nval_dataset   = HierarchicalDataset(val_df, tokenizer, metadata_features, max_length, max_context, augment=False)\ntest_dataset  = HierarchicalDataset(test_df, tokenizer, metadata_features, max_length, max_context, augment=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:53:51.068779Z","iopub.execute_input":"2025-04-21T10:53:51.069054Z","iopub.status.idle":"2025-04-21T10:53:51.277892Z","shell.execute_reply.started":"2025-04-21T10:53:51.069033Z","shell.execute_reply":"2025-04-21T10:53:51.277261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epsilon = 1e-8\nclass ClassBalancedFocalLoss(nn.Module):\n    def __init__(self, beta: float = 0.999, gamma: float = 2.0, reduction='mean'):\n        super().__init__()\n        self.beta = beta\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, logits, labels):\n        labels = labels.view(-1, 1)\n        num_pos = torch.sum(labels == 1).item()\n        num_neg = torch.sum(labels == 0).item()\n        effective_num_pos = 1.0 - np.power(self.beta, num_pos)\n        effective_num_neg = 1.0 - np.power(self.beta, num_neg)\n\n        weights = torch.tensor([\n            (1 - self.beta) / (effective_num_neg + epsilon),\n            (1 - self.beta) / (effective_num_pos + epsilon)\n        ], dtype=torch.float).to(logits.device)\n\n        probs = torch.sigmoid(logits)\n        pt = torch.where(labels == 1, probs, 1 - probs)\n        logpt = torch.log(pt + 1e-9)\n        focal_term = (1 - pt) ** self.gamma\n        loss = -weights[labels.squeeze().long()] * focal_term.squeeze() * logpt.squeeze()\n\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:53:53.376772Z","iopub.execute_input":"2025-04-21T10:53:53.377037Z","iopub.status.idle":"2025-04-21T10:53:53.385762Z","shell.execute_reply.started":"2025-04-21T10:53:53.377017Z","shell.execute_reply":"2025-04-21T10:53:53.385120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MetadataAttention(nn.Module):\n    def __init__(self, input_dim, attn_hidden=32):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, attn_hidden),\n            nn.Tanh(),\n            nn.Linear(attn_hidden, input_dim)\n        )\n        \n    def forward(self, x):\n        # x: (batch_size, input_dim)\n        attn_weights = F.softmax(self.fc(x), dim=1)\n        return x * attn_weights\n\nclass HierarchicalDeceptionModel(nn.Module):\n    def __init__(self, roberta_model_name='roberta-base', metadata_dim=12, max_context=5):\n        super().__init__()\n        print(\"[Model] Loading RoBERTa model...\")\n        self.encoder = RobertaModel.from_pretrained(roberta_model_name)\n        self.hidden_dim = self.encoder.config.hidden_size\n        self.max_context = max_context\n\n        # Multi-head attention for context messages.\n        self.context_attention = nn.MultiheadAttention(embed_dim=self.hidden_dim, num_heads=4, batch_first=True)\n        self.context_proj = nn.Linear(self.hidden_dim, self.hidden_dim)\n\n        # Metadata network with attention.\n        self.metadata_fc = nn.Sequential(\n            nn.Linear(metadata_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 64)\n        )\n        self.metadata_attn = MetadataAttention(64, attn_hidden=32)\n\n        # Final output head.\n        self.output_head = nn.Sequential(\n            nn.Linear(self.hidden_dim * 2 + 64, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, input_ids, attention_mask, context_input_ids, context_attention_mask, metadata):\n        # Encode current message ([CLS] token representation).\n        curr_out = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n\n        # Encode context messages.\n        B, T, L = context_input_ids.shape\n        context_input_ids = context_input_ids.view(B * T, L)\n        context_attention_mask = context_attention_mask.view(B * T, L)\n        ctx_out = self.encoder(input_ids=context_input_ids, attention_mask=context_attention_mask).last_hidden_state[:, 0, :]\n        ctx_out = ctx_out.view(B, T, -1)\n\n        # Apply multi-head attention over context.\n        context_attn_out, _ = self.context_attention(ctx_out, ctx_out, ctx_out)\n        context_repr = torch.mean(context_attn_out, dim=1)\n        context_repr = self.context_proj(context_repr)\n\n        # Process metadata.\n        meta_features = self.metadata_fc(metadata)\n        meta_attended = self.metadata_attn(meta_features)\n\n        # Concatenate features and compute logits.\n        combined = torch.cat([curr_out, context_repr, meta_attended], dim=1)\n        logits = self.output_head(combined)\n        return logits, context_attn_out\n\nprint(\"[Model] HierarchicalDeceptionModel updated and ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:53:54.737786Z","iopub.execute_input":"2025-04-21T10:53:54.738058Z","iopub.status.idle":"2025-04-21T10:53:54.749999Z","shell.execute_reply.started":"2025-04-21T10:53:54.738038Z","shell.execute_reply":"2025-04-21T10:53:54.749258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DeceptionTrainer:\n    def __init__(self, model, train_loader, val_loader, device, loss_fn, optimizer, scheduler=None, save_dir=\"checkpoints\"):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.best_macro_f1 = -1\n        self.save_dir = save_dir\n        os.makedirs(self.save_dir, exist_ok=True)\n\n    def train_epoch(self):\n        self.model.train()\n        total_loss, correct, total = 0.0, 0, 0\n        for batch in tqdm(self.train_loader, desc=\"[Train]\"):\n            self.optimizer.zero_grad()\n            inputs, labels = batch\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n            \n\n            outputs, _ = self.model(**inputs)\n\n            labels = labels.to(self.device)\n            # outputs, _ = self.model(**inputs)\n            loss = self.loss_fn(outputs, labels)\n            loss.backward()\n            self.optimizer.step()\n            if self.scheduler:\n                self.scheduler.step()\n\n            total_loss += loss.item() * labels.size(0)\n            preds = (torch.sigmoid(outputs) >= 0.5).float()\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n        return total_loss / total, correct / total\n\n    def evaluate(self):\n        self.model.eval()\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in tqdm(self.val_loader, desc=\"[Validate]\"):\n                inputs, labels = batch\n                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n                labels = labels.to(self.device)\n                outputs, _ = self.model(**inputs)\n                preds = (torch.sigmoid(outputs) >= 0.5).float()\n                all_preds.extend(preds.cpu().numpy().flatten())\n                all_labels.extend(labels.cpu().numpy().flatten())\n        return self._compute_metrics(all_labels, all_preds)\n\n    def _compute_metrics(self, y_true, y_pred):\n        acc = accuracy_score(y_true, y_pred)\n        prec = precision_score(y_true, y_pred, zero_division=0)\n        rec = recall_score(y_true, y_pred, zero_division=0)\n        f1 = f1_score(y_true, y_pred, zero_division=0)\n        macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n        cm = confusion_matrix(y_true, y_pred)\n        print(\"[Metrics] Accuracy:\", acc, \"| F1:\", f1, \"| Macro-F1:\", macro)\n        return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"macro_f1\": macro, \"confusion_matrix\": cm.tolist()}\n\n    def save_checkpoint(self, epoch, metrics):\n        save_path = os.path.join(self.save_dir, f\"epoch_{epoch}.pt\")\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state_dict\": self.model.state_dict(),\n            \"optimizer_state_dict\": self.optimizer.state_dict(),\n            \"scheduler_state_dict\": self.scheduler.state_dict() if self.scheduler else None,\n            \"metrics\": metrics\n        }, save_path)\n        print(f\"[Checkpoint] Saved model checkpoint to {save_path}\")\n        \n#########################################\n# Optimization Setup\n#########################################\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"[Main] Using device: {device}\")\n\nmetadata_dim = len(metadata_features)\nmodel = HierarchicalDeceptionModel(metadata_dim=metadata_dim, max_context=max_context)\n\n# Loss: You can keep the Class-Balanced Focal Loss as defined.\nloss_fn = ClassBalancedFocalLoss(beta=0.999, gamma=2.0, reduction='mean')\n\n# Use AdamW with weight decay.\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n\n# Scheduler: set up linear warmup and decay.\nnum_train_steps = len(train_loader) * 5  # assuming 5 epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*num_train_steps),\n                                            num_training_steps=num_train_steps)\n\ntrainer = DeceptionTrainer(model, train_loader, val_loader, device, loss_fn, optimizer, scheduler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:53:57.374600Z","iopub.execute_input":"2025-04-21T10:53:57.375091Z","iopub.status.idle":"2025-04-21T10:53:57.800810Z","shell.execute_reply.started":"2025-04-21T10:53:57.375069Z","shell.execute_reply":"2025-04-21T10:53:57.800256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random \nnum_epochs = 5\nos.makedirs(\"models\", exist_ok=True)\n\nfor epoch in range(1, num_epochs + 1):\n    print(f\"[Train] Epoch {epoch}/{num_epochs} started.\")\n    train_loss, train_acc = trainer.train_epoch()\n    print(f\"[Train] Epoch {epoch} finished. Avg Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n    \n    metrics = trainer.evaluate()\n    macro_f1 = metrics[\"macro_f1\"]\n    print(f\"[Validation] Epoch {epoch}: Loss: {train_loss:.4f}, Acc: {metrics['accuracy']:.4f}, Macro-F1: {macro_f1:.4f}\")\n    \n    if macro_f1 > trainer.best_macro_f1:\n        trainer.best_macro_f1 = macro_f1\n        torch.save(model.state_dict(), \"models/hierarchical_model2someimp_best.pt\")\n        print(f\"[Train] Model improved! Best Macro-F1: {macro_f1:.4f}\")\n    else:\n        print(\"[Train] No improvement for this epoch.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T11:00:21.382553Z","iopub.execute_input":"2025-04-21T11:00:21.382825Z","iopub.status.idle":"2025-04-21T12:05:57.519709Z","shell.execute_reply.started":"2025-04-21T11:00:21.382806Z","shell.execute_reply":"2025-04-21T12:05:57.518439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"[Main] Loading best model for test evaluation...\")\nmodel.load_state_dict(torch.load(\"/kaggle/working/models/hierarchical_model2someimp_best.pt\"))\nmodel.to(device)\nmodel.eval()\n\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"[Test Evaluation]\"):\n        inputs, labels = batch\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        labels = labels.to(device)\n        outputs, _ = model(**inputs)\n        preds = (torch.sigmoid(outputs) >= 0.5).float()\n        all_preds.extend(preds.cpu().numpy().flatten())\n        all_labels.extend(labels.cpu().numpy().flatten())\n\ntest_acc = accuracy_score(all_labels, all_preds)\ntest_prec = precision_score(all_labels, all_preds, zero_division=0)\ntest_rec = recall_score(all_labels, all_preds, zero_division=0)\ntest_f1 = f1_score(all_labels, all_preds, zero_division=0)\ntest_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\ntest_cm = confusion_matrix(all_labels, all_preds)\n\nprint(\"[Evaluate] Metrics on test set:\")\nprint(f\"  Accuracy: {test_acc:.4f}\")\nprint(f\"  Precision: {test_prec:.4f}\")\nprint(f\"  Recall: {test_rec:.4f}\")\nprint(f\"  F1 Score: {test_f1:.4f}\")\nprint(f\"  Macro F1 Score: {test_macro:.4f}\")\nprint(f\"  Confusion Matrix:\\n{test_cm}\")\n\nos.makedirs(\"models\", exist_ok=True)\ntorch.save(model.state_dict(), \"models/hierarchical_model2some_final.pt\")\ntokenizer.save_pretrained(\"models/tokenizer2some\")\nprint(\"[Main] Final model and tokenizer saved.\")\nprint(\"[Main] Process completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T12:05:57.521747Z","iopub.execute_input":"2025-04-21T12:05:57.522078Z","iopub.status.idle":"2025-04-21T12:06:37.016256Z","shell.execute_reply.started":"2025-04-21T12:05:57.522049Z","shell.execute_reply":"2025-04-21T12:06:37.015454Z"}},"outputs":[],"execution_count":null}]}